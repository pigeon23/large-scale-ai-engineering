START TIME: Tue Nov 18 22:54:49 CET 2025
END TIME: Tue Nov 18 22:54:49 CET 2025
[sbatch-master] running on nid006572
[sbatch-master] SLURM_NODELIST: nid[006572,007629]
[sbatch-master] SLURM_NNODES: 2
[sbatch-master] SLURM_NODEID: 0
[sbatch-master] define some env vars that will be passed to the compute nodes
[sbatch-master] execute command on compute nodes
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
[srun] rank=0 host=nid006572 noderank=0 localrank=0
W1118 22:55:08.877000 217891 torch/distributed/run.py:792] 
W1118 22:55:08.877000 217891 torch/distributed/run.py:792] *****************************************
W1118 22:55:08.877000 217891 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1118 22:55:08.877000 217891 torch/distributed/run.py:792] *****************************************
[srun] rank=1 host=nid007629 noderank=1 localrank=0
W1118 22:55:17.148000 178733 torch/distributed/run.py:792] 
W1118 22:55:17.148000 178733 torch/distributed/run.py:792] *****************************************
W1118 22:55:17.148000 178733 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1118 22:55:17.148000 178733 torch/distributed/run.py:792] *****************************************
2025-11-18 22:55:26,830 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=4096, batch_size=16, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=5, training_steps=300, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False)
2025-11-18 22:55:26,830 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=4096, batch_size=16, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=5, training_steps=300, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False)
2025-11-18 22:55:26,830 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=4096, batch_size=16, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=5, training_steps=300, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False)
2025-11-18 22:55:26,831 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=4096, batch_size=16, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=5, training_steps=300, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False)
2025-11-18 22:55:26,832 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=4096, batch_size=16, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=5, training_steps=300, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False)
2025-11-18 22:55:26,832 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=4096, batch_size=16, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=5, training_steps=300, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False)
2025-11-18 22:55:26,832 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=4096, batch_size=16, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=5, training_steps=300, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False)
2025-11-18 22:55:26,832 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=4096, batch_size=16, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=5, training_steps=300, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False)
2025-11-18 22:55:32,266 - root - INFO - Setting up DataLoaders...
2025-11-18 22:55:32,332 - root - INFO - Setting up DataLoaders...
2025-11-18 22:55:32,642 - root - INFO - Setting up DataLoaders...
2025-11-18 22:55:32,646 - root - INFO - Setting up DataLoaders...
2025-11-18 22:55:32,648 - root - INFO - Setting up DataLoaders...
2025-11-18 22:55:32,654 - root - INFO - Setting up DataLoaders...
2025-11-18 22:55:32,670 - root - INFO - Setting up DataLoaders...
2025-11-18 22:55:32,671 - root - INFO - Setting up DataLoaders...
2025-11-18 22:55:42,881 - root - INFO - Setting up Model...
2025-11-18 22:55:42,881 - root - INFO - Setting up Model...
2025-11-18 22:55:42,881 - root - INFO - Setting up Model...
2025-11-18 22:55:42,882 - root - INFO - Setting up Model...
2025-11-18 22:55:42,911 - root - INFO - Setting up Model...
2025-11-18 22:55:42,911 - root - INFO - Setting up Model...
2025-11-18 22:55:42,911 - root - INFO - Setting up Model...
2025-11-18 22:55:42,912 - root - INFO - Setting up Model...
2025-11-18 22:55:50,063 - root - INFO - Applying Tensor Parallelism with size 4...
2025-11-18 22:55:50,149 - root - INFO - Applying Tensor Parallelism with size 4...
2025-11-18 22:55:50,228 - root - INFO - Applying Tensor Parallelism with size 4...
2025-11-18 22:55:50,240 - root - INFO - Applying Tensor Parallelism with size 4...
2025-11-18 22:55:50,240 - root - INFO - Applying Tensor Parallelism with size 4...
2025-11-18 22:55:50,246 - root - INFO - Applying Tensor Parallelism with size 4...
2025-11-18 22:55:50,353 - root - INFO - Applying Tensor Parallelism with size 4...
2025-11-18 22:55:50,386 - root - INFO - Applying Tensor Parallelism with size 4...
2025-11-18 22:55:55,668 - root - INFO - Starting training!
2025-11-18 22:55:55,669 - root - INFO - Starting training!
2025-11-18 22:55:55,670 - root - INFO - Starting training!
2025-11-18 22:55:55,671 - root - INFO - Starting training!
2025-11-18 22:55:55,712 - root - INFO - Starting training!
2025-11-18 22:55:55,712 - root - INFO - Starting training!
2025-11-18 22:55:55,714 - root - INFO - Starting training!
2025-11-18 22:55:55,717 - root - INFO - Starting training!
2025-11-18 22:56:03,660 - root - INFO - Step: 1 | Loss (Avg): 11.95 | Tokens per second: 8200.97 | Training tokens per second (%): 15.59 | MFU (%): 5.97 | TFLOPs: 59.03
2025-11-18 22:56:07,018 - root - INFO - Step: 5 | Loss (Avg): 11.30 | Tokens per second: 78075.92 | Training tokens per second (%): 23.80 | MFU (%): 56.82 | TFLOPs: 561.97
2025-11-18 22:56:11,200 - root - INFO - Step: 10 | Loss (Avg): 9.87 | Tokens per second: 78367.49 | Training tokens per second (%): 20.80 | MFU (%): 57.03 | TFLOPs: 564.07
2025-11-18 22:56:15,249 - root - INFO - Step: 15 | Loss (Avg): 9.39 | Tokens per second: 80934.55 | Training tokens per second (%): 25.33 | MFU (%): 58.90 | TFLOPs: 582.55
2025-11-18 22:56:19,296 - root - INFO - Step: 20 | Loss (Avg): 8.95 | Tokens per second: 80972.25 | Training tokens per second (%): 19.61 | MFU (%): 58.93 | TFLOPs: 582.82
2025-11-18 22:56:23,540 - root - INFO - Step: 25 | Loss (Avg): 8.51 | Tokens per second: 77197.82 | Training tokens per second (%): 22.65 | MFU (%): 56.18 | TFLOPs: 555.65
2025-11-18 22:56:27,544 - root - INFO - Step: 30 | Loss (Avg): 8.28 | Tokens per second: 81858.33 | Training tokens per second (%): 20.37 | MFU (%): 59.57 | TFLOPs: 589.19
2025-11-18 22:56:31,636 - root - INFO - Step: 35 | Loss (Avg): 7.88 | Tokens per second: 80072.26 | Training tokens per second (%): 23.08 | MFU (%): 58.27 | TFLOPs: 576.34
2025-11-18 22:56:35,646 - root - INFO - Step: 40 | Loss (Avg): 7.70 | Tokens per second: 81725.19 | Training tokens per second (%): 21.75 | MFU (%): 59.48 | TFLOPs: 588.24
2025-11-18 22:56:39,649 - root - INFO - Step: 45 | Loss (Avg): 7.54 | Tokens per second: 81872.35 | Training tokens per second (%): 20.17 | MFU (%): 59.58 | TFLOPs: 589.30
2025-11-18 22:56:43,672 - root - INFO - Step: 50 | Loss (Avg): 7.42 | Tokens per second: 81436.83 | Training tokens per second (%): 22.51 | MFU (%): 59.27 | TFLOPs: 586.16
2025-11-18 22:56:47,659 - root - INFO - Step: 55 | Loss (Avg): 7.28 | Tokens per second: 82211.32 | Training tokens per second (%): 23.00 | MFU (%): 59.83 | TFLOPs: 591.74
2025-11-18 22:56:51,627 - root - INFO - Step: 60 | Loss (Avg): 7.14 | Tokens per second: 82574.32 | Training tokens per second (%): 19.10 | MFU (%): 60.10 | TFLOPs: 594.35
2025-11-18 22:56:55,599 - root - INFO - Step: 65 | Loss (Avg): 7.32 | Tokens per second: 82497.12 | Training tokens per second (%): 18.10 | MFU (%): 60.04 | TFLOPs: 593.79
2025-11-18 22:56:59,617 - root - INFO - Step: 70 | Loss (Avg): 7.30 | Tokens per second: 81562.47 | Training tokens per second (%): 23.32 | MFU (%): 59.36 | TFLOPs: 587.06
2025-11-18 22:57:03,668 - root - INFO - Step: 75 | Loss (Avg): 7.01 | Tokens per second: 80885.52 | Training tokens per second (%): 22.05 | MFU (%): 58.87 | TFLOPs: 582.19
2025-11-18 22:57:07,639 - root - INFO - Step: 80 | Loss (Avg): 7.03 | Tokens per second: 82536.95 | Training tokens per second (%): 18.54 | MFU (%): 60.07 | TFLOPs: 594.08
2025-11-18 22:57:11,736 - root - INFO - Step: 85 | Loss (Avg): 7.05 | Tokens per second: 79980.13 | Training tokens per second (%): 23.01 | MFU (%): 58.21 | TFLOPs: 575.68
2025-11-18 22:57:15,758 - root - INFO - Step: 90 | Loss (Avg): 7.06 | Tokens per second: 81471.67 | Training tokens per second (%): 24.03 | MFU (%): 59.29 | TFLOPs: 586.41
2025-11-18 22:57:19,772 - root - INFO - Step: 95 | Loss (Avg): 7.06 | Tokens per second: 81647.51 | Training tokens per second (%): 19.53 | MFU (%): 59.42 | TFLOPs: 587.68
2025-11-18 22:57:23,791 - root - INFO - Step: 100 | Loss (Avg): 6.95 | Tokens per second: 81526.58 | Training tokens per second (%): 19.34 | MFU (%): 59.33 | TFLOPs: 586.81
2025-11-18 22:57:27,781 - root - INFO - Step: 105 | Loss (Avg): 6.98 | Tokens per second: 82144.01 | Training tokens per second (%): 20.69 | MFU (%): 59.78 | TFLOPs: 591.25
2025-11-18 22:57:31,774 - root - INFO - Step: 110 | Loss (Avg): 7.05 | Tokens per second: 82052.81 | Training tokens per second (%): 17.77 | MFU (%): 59.72 | TFLOPs: 590.59
2025-11-18 22:57:35,762 - root - INFO - Step: 115 | Loss (Avg): 7.07 | Tokens per second: 82172.47 | Training tokens per second (%): 21.36 | MFU (%): 59.80 | TFLOPs: 591.46
2025-11-18 22:57:39,903 - root - INFO - Step: 120 | Loss (Avg): 6.94 | Tokens per second: 79130.58 | Training tokens per second (%): 21.91 | MFU (%): 57.59 | TFLOPs: 569.56
2025-11-18 22:57:43,856 - root - INFO - Step: 125 | Loss (Avg): 6.80 | Tokens per second: 82914.67 | Training tokens per second (%): 18.69 | MFU (%): 60.34 | TFLOPs: 596.80
2025-11-18 22:57:47,862 - root - INFO - Step: 130 | Loss (Avg): 6.89 | Tokens per second: 81786.60 | Training tokens per second (%): 19.96 | MFU (%): 59.52 | TFLOPs: 588.68
2025-11-18 22:57:51,833 - root - INFO - Step: 135 | Loss (Avg): 6.77 | Tokens per second: 82524.98 | Training tokens per second (%): 18.30 | MFU (%): 60.06 | TFLOPs: 593.99
2025-11-18 22:57:55,851 - root - INFO - Step: 140 | Loss (Avg): 6.79 | Tokens per second: 81574.18 | Training tokens per second (%): 22.36 | MFU (%): 59.37 | TFLOPs: 587.15
2025-11-18 22:57:59,879 - root - INFO - Step: 145 | Loss (Avg): 6.71 | Tokens per second: 81351.27 | Training tokens per second (%): 22.10 | MFU (%): 59.21 | TFLOPs: 585.54
2025-11-18 22:58:03,867 - root - INFO - Step: 150 | Loss (Avg): 6.70 | Tokens per second: 82155.55 | Training tokens per second (%): 17.16 | MFU (%): 59.79 | TFLOPs: 591.33
2025-11-18 22:58:07,866 - root - INFO - Step: 155 | Loss (Avg): 6.77 | Tokens per second: 81957.84 | Training tokens per second (%): 21.41 | MFU (%): 59.65 | TFLOPs: 589.91
2025-11-18 22:58:11,863 - root - INFO - Step: 160 | Loss (Avg): 6.75 | Tokens per second: 81977.17 | Training tokens per second (%): 19.49 | MFU (%): 59.66 | TFLOPs: 590.05
2025-11-18 22:58:16,081 - root - INFO - Step: 165 | Loss (Avg): 6.63 | Tokens per second: 77703.96 | Training tokens per second (%): 23.66 | MFU (%): 56.55 | TFLOPs: 559.29
2025-11-18 22:58:20,079 - root - INFO - Step: 170 | Loss (Avg): 6.66 | Tokens per second: 81960.49 | Training tokens per second (%): 23.37 | MFU (%): 59.65 | TFLOPs: 589.93
2025-11-18 22:58:24,048 - root - INFO - Step: 175 | Loss (Avg): 6.64 | Tokens per second: 82562.13 | Training tokens per second (%): 21.12 | MFU (%): 60.09 | TFLOPs: 594.26
2025-11-18 22:58:28,052 - root - INFO - Step: 180 | Loss (Avg): 6.79 | Tokens per second: 81841.82 | Training tokens per second (%): 23.02 | MFU (%): 59.56 | TFLOPs: 589.08
2025-11-18 22:58:32,094 - root - INFO - Step: 185 | Loss (Avg): 6.75 | Tokens per second: 81068.74 | Training tokens per second (%): 26.38 | MFU (%): 59.00 | TFLOPs: 583.51
2025-11-18 22:58:36,087 - root - INFO - Step: 190 | Loss (Avg): 6.64 | Tokens per second: 82074.08 | Training tokens per second (%): 20.73 | MFU (%): 59.73 | TFLOPs: 590.75
2025-11-18 22:58:40,102 - root - INFO - Step: 195 | Loss (Avg): 6.69 | Tokens per second: 81615.31 | Training tokens per second (%): 22.55 | MFU (%): 59.40 | TFLOPs: 587.45
2025-11-18 22:58:44,132 - root - INFO - Step: 200 | Loss (Avg): 6.72 | Tokens per second: 81312.25 | Training tokens per second (%): 24.23 | MFU (%): 59.18 | TFLOPs: 585.26
2025-11-18 22:58:48,188 - root - INFO - Step: 205 | Loss (Avg): 6.61 | Tokens per second: 80788.22 | Training tokens per second (%): 27.38 | MFU (%): 58.80 | TFLOPs: 581.49
2025-11-18 22:58:52,196 - root - INFO - Step: 210 | Loss (Avg): 6.52 | Tokens per second: 81779.46 | Training tokens per second (%): 24.07 | MFU (%): 59.52 | TFLOPs: 588.63
2025-11-18 22:58:56,179 - root - INFO - Step: 215 | Loss (Avg): 6.47 | Tokens per second: 82263.34 | Training tokens per second (%): 20.41 | MFU (%): 59.87 | TFLOPs: 592.11
2025-11-18 22:59:00,150 - root - INFO - Step: 220 | Loss (Avg): 6.50 | Tokens per second: 82518.89 | Training tokens per second (%): 22.46 | MFU (%): 60.06 | TFLOPs: 593.95
2025-11-18 22:59:04,165 - root - INFO - Step: 225 | Loss (Avg): 6.63 | Tokens per second: 81622.00 | Training tokens per second (%): 27.31 | MFU (%): 59.40 | TFLOPs: 587.49
2025-11-18 22:59:08,167 - root - INFO - Step: 230 | Loss (Avg): 6.71 | Tokens per second: 81882.03 | Training tokens per second (%): 18.26 | MFU (%): 59.59 | TFLOPs: 589.36
2025-11-18 22:59:12,193 - root - INFO - Step: 235 | Loss (Avg): 6.57 | Tokens per second: 81395.44 | Training tokens per second (%): 23.36 | MFU (%): 59.24 | TFLOPs: 585.86
2025-11-18 22:59:16,159 - root - INFO - Step: 240 | Loss (Avg): 6.52 | Tokens per second: 82632.16 | Training tokens per second (%): 19.62 | MFU (%): 60.14 | TFLOPs: 594.76
2025-11-18 22:59:20,171 - root - INFO - Step: 245 | Loss (Avg): 6.53 | Tokens per second: 81672.83 | Training tokens per second (%): 24.61 | MFU (%): 59.44 | TFLOPs: 587.86
2025-11-18 22:59:24,181 - root - INFO - Step: 250 | Loss (Avg): 6.41 | Tokens per second: 81734.31 | Training tokens per second (%): 22.54 | MFU (%): 59.48 | TFLOPs: 588.30
2025-11-18 22:59:28,172 - root - INFO - Step: 255 | Loss (Avg): 6.48 | Tokens per second: 82097.41 | Training tokens per second (%): 18.93 | MFU (%): 59.75 | TFLOPs: 590.92
2025-11-18 22:59:32,148 - root - INFO - Step: 260 | Loss (Avg): 6.54 | Tokens per second: 82411.89 | Training tokens per second (%): 20.07 | MFU (%): 59.98 | TFLOPs: 593.18
2025-11-18 22:59:36,152 - root - INFO - Step: 265 | Loss (Avg): 6.59 | Tokens per second: 81851.35 | Training tokens per second (%): 23.74 | MFU (%): 59.57 | TFLOPs: 589.14
2025-11-18 22:59:40,203 - root - INFO - Step: 270 | Loss (Avg): 6.48 | Tokens per second: 80890.12 | Training tokens per second (%): 27.02 | MFU (%): 58.87 | TFLOPs: 582.23
2025-11-18 22:59:44,202 - root - INFO - Step: 275 | Loss (Avg): 6.46 | Tokens per second: 81946.05 | Training tokens per second (%): 22.00 | MFU (%): 59.64 | TFLOPs: 589.83
2025-11-18 22:59:48,199 - root - INFO - Step: 280 | Loss (Avg): 6.45 | Tokens per second: 81983.89 | Training tokens per second (%): 19.91 | MFU (%): 59.67 | TFLOPs: 590.10
2025-11-18 22:59:52,277 - root - INFO - Step: 285 | Loss (Avg): 6.35 | Tokens per second: 80369.31 | Training tokens per second (%): 20.26 | MFU (%): 58.49 | TFLOPs: 578.48
2025-11-18 22:59:56,266 - root - INFO - Step: 290 | Loss (Avg): 6.45 | Tokens per second: 82146.07 | Training tokens per second (%): 23.67 | MFU (%): 59.78 | TFLOPs: 591.27
2025-11-18 23:00:00,281 - root - INFO - Step: 295 | Loss (Avg): 6.36 | Tokens per second: 81621.55 | Training tokens per second (%): 24.23 | MFU (%): 59.40 | TFLOPs: 587.49
2025-11-18 23:00:04,296 - root - INFO - Training completed
2025-11-18 23:00:04,299 - root - INFO - Step: 300 | Loss (Avg): 6.51 | Tokens per second: 81545.17 | Training tokens per second (%): 26.85 | MFU (%): 59.35 | TFLOPs: 586.94
2025-11-18 23:00:04,299 - root - INFO - Training completed
2025-11-18 23:00:04,300 - root - INFO - Training completed
2025-11-18 23:00:04,301 - root - INFO - Training completed
2025-11-18 23:00:04,303 - root - INFO - Training completed
2025-11-18 23:00:04,303 - root - INFO - Training completed
2025-11-18 23:00:04,304 - root - INFO - Training completed
2025-11-18 23:00:04,304 - root - INFO - Training completed
[sbatch-master] task finished
