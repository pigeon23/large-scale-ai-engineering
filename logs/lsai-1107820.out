START TIME: Tue Nov 18 22:39:32 CET 2025
END TIME: Tue Nov 18 22:39:32 CET 2025
[sbatch-master] running on nid007372
[sbatch-master] SLURM_NODELIST: nid[007372,007414]
[sbatch-master] SLURM_NNODES: 2
[sbatch-master] SLURM_NODEID: 0
[sbatch-master] define some env vars that will be passed to the compute nodes
[sbatch-master] execute command on compute nodes
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
[srun] rank=0 host=nid007372 noderank=0 localrank=0
W1118 22:39:54.394000 253353 torch/distributed/run.py:792] 
W1118 22:39:54.394000 253353 torch/distributed/run.py:792] *****************************************
W1118 22:39:54.394000 253353 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1118 22:39:54.394000 253353 torch/distributed/run.py:792] *****************************************
[srun] rank=1 host=nid007414 noderank=1 localrank=0
W1118 22:40:04.908000 190325 torch/distributed/run.py:792] 
W1118 22:40:04.908000 190325 torch/distributed/run.py:792] *****************************************
W1118 22:40:04.908000 190325 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1118 22:40:04.908000 190325 torch/distributed/run.py:792] *****************************************
2025-11-18 22:40:12,839 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=4096, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=5, training_steps=100, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False)
2025-11-18 22:40:12,839 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=4096, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=5, training_steps=100, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False)
2025-11-18 22:40:12,840 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=4096, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=5, training_steps=100, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False)
2025-11-18 22:40:12,840 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=4096, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=5, training_steps=100, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False)
2025-11-18 22:40:12,866 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=4096, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=5, training_steps=100, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False)
2025-11-18 22:40:12,866 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=4096, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=5, training_steps=100, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False)
2025-11-18 22:40:12,866 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=4096, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=5, training_steps=100, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False)
2025-11-18 22:40:12,866 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=4096, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=5, training_steps=100, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False)
2025-11-18 22:40:18,290 - root - INFO - Setting up DataLoaders...
2025-11-18 22:40:18,340 - root - INFO - Setting up DataLoaders...
2025-11-18 22:40:18,654 - root - INFO - Setting up DataLoaders...
2025-11-18 22:40:18,661 - root - INFO - Setting up DataLoaders...
2025-11-18 22:40:18,663 - root - INFO - Setting up DataLoaders...
2025-11-18 22:40:18,664 - root - INFO - Setting up DataLoaders...
2025-11-18 22:40:18,664 - root - INFO - Setting up DataLoaders...
2025-11-18 22:40:18,669 - root - INFO - Setting up DataLoaders...
2025-11-18 22:40:24,106 - root - INFO - Setting up Model...
2025-11-18 22:40:24,134 - root - INFO - Setting up Model...
2025-11-18 22:40:24,134 - root - INFO - Setting up Model...
2025-11-18 22:40:24,160 - root - INFO - Setting up Model...
2025-11-18 22:40:24,234 - root - INFO - Setting up Model...
2025-11-18 22:40:24,288 - root - INFO - Setting up Model...
2025-11-18 22:40:24,388 - root - INFO - Setting up Model...
2025-11-18 22:40:24,444 - root - INFO - Setting up Model...
2025-11-18 22:40:31,429 - root - INFO - Applying Tensor Parallelism with size 2...
2025-11-18 22:40:31,472 - root - INFO - Applying Tensor Parallelism with size 2...
2025-11-18 22:40:31,500 - root - INFO - Applying Tensor Parallelism with size 2...
2025-11-18 22:40:31,581 - root - INFO - Applying Tensor Parallelism with size 2...
2025-11-18 22:40:31,703 - root - INFO - Applying Tensor Parallelism with size 2...
2025-11-18 22:40:31,792 - root - INFO - Applying Tensor Parallelism with size 2...
2025-11-18 22:40:33,707 - root - INFO - Applying Tensor Parallelism with size 2...
2025-11-18 22:40:34,018 - root - INFO - Applying Tensor Parallelism with size 2...
2025-11-18 22:40:34,791 - root - INFO - Starting training!
2025-11-18 22:40:34,792 - root - INFO - Starting training!
2025-11-18 22:40:34,826 - root - INFO - Starting training!
2025-11-18 22:40:34,831 - root - INFO - Starting training!
2025-11-18 22:40:35,255 - root - INFO - Starting training!
2025-11-18 22:40:35,263 - root - INFO - Starting training!
2025-11-18 22:40:35,288 - root - INFO - Starting training!
2025-11-18 22:40:35,297 - root - INFO - Starting training!
2025-11-18 22:40:39,448 - root - INFO - Step: 1 | Loss (Avg): 11.95 | Tokens per second: 7876.86 | Training tokens per second (%): 19.55 | MFU (%): 5.73 | TFLOPs: 56.70
2025-11-18 22:40:42,748 - root - INFO - Step: 5 | Loss (Avg): 11.21 | Tokens per second: 39718.39 | Training tokens per second (%): 26.90 | MFU (%): 28.91 | TFLOPs: 285.88
2025-11-18 22:40:46,882 - root - INFO - Step: 10 | Loss (Avg): 9.82 | Tokens per second: 39639.31 | Training tokens per second (%): 24.17 | MFU (%): 28.85 | TFLOPs: 285.31
2025-11-18 22:40:50,853 - root - INFO - Step: 15 | Loss (Avg): 9.30 | Tokens per second: 41255.16 | Training tokens per second (%): 22.83 | MFU (%): 30.02 | TFLOPs: 296.94
2025-11-18 22:40:54,881 - root - INFO - Step: 20 | Loss (Avg): 8.93 | Tokens per second: 40681.31 | Training tokens per second (%): 24.43 | MFU (%): 29.61 | TFLOPs: 292.81
2025-11-18 22:40:58,826 - root - INFO - Step: 25 | Loss (Avg): 8.47 | Tokens per second: 41535.46 | Training tokens per second (%): 21.14 | MFU (%): 30.23 | TFLOPs: 298.96
2025-11-18 22:41:02,767 - root - INFO - Step: 30 | Loss (Avg): 8.13 | Tokens per second: 41571.67 | Training tokens per second (%): 20.92 | MFU (%): 30.25 | TFLOPs: 299.22
2025-11-18 22:41:06,820 - root - INFO - Step: 35 | Loss (Avg): 7.94 | Tokens per second: 40431.65 | Training tokens per second (%): 18.63 | MFU (%): 29.43 | TFLOPs: 291.02
2025-11-18 22:41:10,773 - root - INFO - Step: 40 | Loss (Avg): 7.63 | Tokens per second: 41444.24 | Training tokens per second (%): 18.74 | MFU (%): 30.16 | TFLOPs: 298.30
2025-11-18 22:41:14,970 - root - INFO - Step: 45 | Loss (Avg): 7.48 | Tokens per second: 39041.22 | Training tokens per second (%): 23.35 | MFU (%): 28.41 | TFLOPs: 281.01
2025-11-18 22:41:18,931 - root - INFO - Step: 50 | Loss (Avg): 7.36 | Tokens per second: 41365.35 | Training tokens per second (%): 19.41 | MFU (%): 30.10 | TFLOPs: 297.74
2025-11-18 22:41:22,874 - root - INFO - Step: 55 | Loss (Avg): 7.35 | Tokens per second: 41558.57 | Training tokens per second (%): 23.46 | MFU (%): 30.25 | TFLOPs: 299.13
2025-11-18 22:41:26,839 - root - INFO - Step: 60 | Loss (Avg): 7.29 | Tokens per second: 41314.56 | Training tokens per second (%): 24.94 | MFU (%): 30.07 | TFLOPs: 297.37
2025-11-18 22:41:30,795 - root - INFO - Step: 65 | Loss (Avg): 7.29 | Tokens per second: 41421.87 | Training tokens per second (%): 17.84 | MFU (%): 30.15 | TFLOPs: 298.14
2025-11-18 22:41:34,749 - root - INFO - Step: 70 | Loss (Avg): 7.05 | Tokens per second: 41444.76 | Training tokens per second (%): 18.75 | MFU (%): 30.16 | TFLOPs: 298.31
2025-11-18 22:41:38,697 - root - INFO - Step: 75 | Loss (Avg): 7.00 | Tokens per second: 41498.26 | Training tokens per second (%): 17.40 | MFU (%): 30.20 | TFLOPs: 298.69
2025-11-18 22:41:42,668 - root - INFO - Step: 80 | Loss (Avg): 7.24 | Tokens per second: 41264.43 | Training tokens per second (%): 24.71 | MFU (%): 30.03 | TFLOPs: 297.01
2025-11-18 22:41:46,662 - root - INFO - Step: 85 | Loss (Avg): 7.06 | Tokens per second: 41018.30 | Training tokens per second (%): 25.28 | MFU (%): 29.85 | TFLOPs: 295.24
2025-11-18 22:41:50,588 - root - INFO - Step: 90 | Loss (Avg): 7.01 | Tokens per second: 41736.40 | Training tokens per second (%): 14.79 | MFU (%): 30.37 | TFLOPs: 300.41
2025-11-18 22:41:54,594 - root - INFO - Step: 95 | Loss (Avg): 7.02 | Tokens per second: 40895.31 | Training tokens per second (%): 20.99 | MFU (%): 29.76 | TFLOPs: 294.35
2025-11-18 22:41:58,592 - root - INFO - Training completed
2025-11-18 22:41:58,595 - root - INFO - Training completed
2025-11-18 22:41:58,596 - root - INFO - Training completed
2025-11-18 22:41:58,597 - root - INFO - Training completed
2025-11-18 22:41:58,597 - root - INFO - Step: 100 | Loss (Avg): 6.95 | Tokens per second: 40935.04 | Training tokens per second (%): 25.99 | MFU (%): 29.79 | TFLOPs: 294.64
2025-11-18 22:41:58,597 - root - INFO - Training completed
2025-11-18 22:41:58,599 - root - INFO - Training completed
2025-11-18 22:41:58,602 - root - INFO - Training completed
2025-11-18 22:41:58,619 - root - INFO - Training completed
[sbatch-master] task finished
