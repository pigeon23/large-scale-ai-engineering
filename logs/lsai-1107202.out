START TIME: Tue Nov 18 19:27:20 CET 2025
END TIME: Tue Nov 18 19:27:20 CET 2025
[sbatch-master] running on nid006891
[sbatch-master] SLURM_NODELIST: nid006891
[sbatch-master] SLURM_NNODES: 1
[sbatch-master] SLURM_NODEID: 0
[sbatch-master] define some env vars that will be passed to the compute nodes
[sbatch-master] execute command on compute nodes
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
[srun] rank=0 host=nid006891 noderank=0 localrank=0
W1118 19:27:41.417000 34798 torch/distributed/run.py:792] 
W1118 19:27:41.417000 34798 torch/distributed/run.py:792] *****************************************
W1118 19:27:41.417000 34798 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1118 19:27:41.417000 34798 torch/distributed/run.py:792] *****************************************
2025-11-18 19:27:50,567 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=4096, batch_size=4, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=5, training_steps=100, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False)
2025-11-18 19:27:50,568 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=4096, batch_size=4, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=5, training_steps=100, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False)
2025-11-18 19:27:50,568 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=4096, batch_size=4, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=5, training_steps=100, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False)
2025-11-18 19:27:50,569 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=4096, batch_size=4, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=5, training_steps=100, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False)
[Distributed Init] Rank 0 initialized on 0 on GPU 0.
[rank0]:[W1118 19:27:55.190532507 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[Distributed Init] Rank 3 initialized on 0 on GPU 3.
[rank3]:[W1118 19:27:55.212051264 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[Distributed Init] Rank 1 initialized on 0 on GPU 1.
[rank1]:[W1118 19:27:55.302026209 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[Distributed Init] Rank 2 initialized on 0 on GPU 2.
[rank2]:[W1118 19:27:55.312228915 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[Rank 0] All ranks ready!
2025-11-18 19:28:02,874 - root - INFO - Setting up DataLoaders...
2025-11-18 19:28:02,874 - root - INFO - Setting up DataLoaders...
2025-11-18 19:28:02,874 - root - INFO - Setting up DataLoaders...
2025-11-18 19:28:02,874 - root - INFO - Setting up DataLoaders...
2025-11-18 19:28:16,736 - root - INFO - Setting up Model...
2025-11-18 19:28:16,736 - root - INFO - Setting up Model...
2025-11-18 19:28:16,736 - root - INFO - Setting up Model...
2025-11-18 19:28:16,737 - root - INFO - Setting up Model...
[rank3]: Traceback (most recent call last):
[rank3]:   File "/iopsstor/scratch/cscs/yiswang/large-sc/project/train.py", line 111, in <module>
[rank3]:     train(args)
[rank3]:   File "/iopsstor/scratch/cscs/yiswang/large-sc/project/train.py", line 48, in train
[rank3]:     model = DDP(model, device_ids=[local_rank])
[rank3]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/distributed.py", line 742, in __init__
[rank3]:     self._log_and_throw(
[rank3]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/distributed.py", line 1130, in _log_and_throw
[rank3]:     raise err_type(err_msg)
[rank3]: ValueError: DistributedDataParallel device_ids and output_device arguments only work with single-device/multiple-device GPU modules or CPU modules, but got device_ids [3], output_device None, and module parameters {device(type='cpu')}.
[rank2]: Traceback (most recent call last):
[rank2]:   File "/iopsstor/scratch/cscs/yiswang/large-sc/project/train.py", line 111, in <module>
[rank2]:     train(args)
[rank2]:   File "/iopsstor/scratch/cscs/yiswang/large-sc/project/train.py", line 48, in train
[rank2]:     model = DDP(model, device_ids=[local_rank])
[rank2]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/distributed.py", line 742, in __init__
[rank2]:     self._log_and_throw(
[rank2]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/distributed.py", line 1130, in _log_and_throw
[rank2]:     raise err_type(err_msg)
[rank2]: ValueError: DistributedDataParallel device_ids and output_device arguments only work with single-device/multiple-device GPU modules or CPU modules, but got device_ids [2], output_device None, and module parameters {device(type='cpu')}.
[rank0]: Traceback (most recent call last):
[rank0]:   File "/iopsstor/scratch/cscs/yiswang/large-sc/project/train.py", line 111, in <module>
[rank0]:     train(args)
[rank0]:   File "/iopsstor/scratch/cscs/yiswang/large-sc/project/train.py", line 48, in train
[rank0]:     model = DDP(model, device_ids=[local_rank])
[rank0]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/distributed.py", line 742, in __init__
[rank0]:     self._log_and_throw(
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/distributed.py", line 1130, in _log_and_throw
[rank0]:     raise err_type(err_msg)
[rank0]: ValueError: DistributedDataParallel device_ids and output_device arguments only work with single-device/multiple-device GPU modules or CPU modules, but got device_ids [0], output_device None, and module parameters {device(type='cpu')}.
W1118 19:28:50.988000 34798 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 35255 closing signal SIGTERM
W1118 19:28:50.988000 34798 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 35256 closing signal SIGTERM
W1118 19:28:50.989000 34798 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 35257 closing signal SIGTERM
E1118 19:28:51.454000 34798 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 3 (pid: 35258) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.6.0a0+ecf3bae40a.nv25.1', 'console_scripts', 'torchrun')())
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 918, in main
    run(args)
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/iopsstor/scratch/cscs/yiswang/large-sc/project/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-11-18_19:28:50
  host      : nid006891
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 35258)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: nid006891: task 0: Exited with exit code 1
srun: Terminating StepId=1107202.0
