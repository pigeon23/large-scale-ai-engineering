START TIME: Tue Nov 18 22:23:32 CET 2025
END TIME: Tue Nov 18 22:23:32 CET 2025
[sbatch-master] running on nid006665
[sbatch-master] SLURM_NODELIST: nid[006665,006698]
[sbatch-master] SLURM_NNODES: 2
[sbatch-master] SLURM_NODEID: 0
[sbatch-master] define some env vars that will be passed to the compute nodes
[sbatch-master] execute command on compute nodes
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
[srun] rank=0 host=nid006665 noderank=0 localrank=0
W1118 22:23:51.844000 138041 torch/distributed/run.py:792] 
W1118 22:23:51.844000 138041 torch/distributed/run.py:792] *****************************************
W1118 22:23:51.844000 138041 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1118 22:23:51.844000 138041 torch/distributed/run.py:792] *****************************************
[srun] rank=1 host=nid006698 noderank=1 localrank=0
W1118 22:23:57.090000 107200 torch/distributed/run.py:792] 
W1118 22:23:57.090000 107200 torch/distributed/run.py:792] *****************************************
W1118 22:23:57.090000 107200 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1118 22:23:57.090000 107200 torch/distributed/run.py:792] *****************************************
2025-11-18 22:24:05,908 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=4096, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=5, training_steps=100, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False)
2025-11-18 22:24:05,908 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=4096, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=5, training_steps=100, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False)
2025-11-18 22:24:05,908 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=4096, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=5, training_steps=100, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False)
2025-11-18 22:24:05,911 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=4096, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=5, training_steps=100, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False)
2025-11-18 22:24:05,950 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=4096, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=5, training_steps=100, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False)
2025-11-18 22:24:05,950 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=4096, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=5, training_steps=100, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False)
2025-11-18 22:24:05,950 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=4096, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=5, training_steps=100, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False)
2025-11-18 22:24:05,950 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=4096, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=5, training_steps=100, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False)
2025-11-18 22:24:13,512 - root - INFO - Setting up DataLoaders...
2025-11-18 22:24:13,827 - root - INFO - Setting up DataLoaders...
2025-11-18 22:24:13,866 - root - INFO - Setting up DataLoaders...
2025-11-18 22:24:13,872 - root - INFO - Setting up DataLoaders...
2025-11-18 22:24:14,800 - root - INFO - Setting up DataLoaders...
2025-11-18 22:24:15,198 - root - INFO - Setting up DataLoaders...
2025-11-18 22:24:15,200 - root - INFO - Setting up DataLoaders...
2025-11-18 22:24:15,216 - root - INFO - Setting up DataLoaders...
2025-11-18 22:24:23,726 - root - INFO - Setting up Model...
2025-11-18 22:24:23,726 - root - INFO - Setting up Model...
2025-11-18 22:24:23,727 - root - INFO - Setting up Model...
2025-11-18 22:24:23,727 - root - INFO - Setting up Model...
2025-11-18 22:24:26,497 - root - INFO - Setting up Model...
2025-11-18 22:24:26,497 - root - INFO - Setting up Model...
2025-11-18 22:24:26,497 - root - INFO - Setting up Model...
2025-11-18 22:24:26,505 - root - INFO - Setting up Model...
2025-11-18 22:24:29,982 - root - INFO - Applying Tensor Parallelism with size 2...
2025-11-18 22:24:30,023 - root - INFO - Applying Tensor Parallelism with size 2...
2025-11-18 22:24:30,235 - root - INFO - Applying Tensor Parallelism with size 2...
2025-11-18 22:24:30,303 - root - INFO - Applying Tensor Parallelism with size 2...
2025-11-18 22:24:32,881 - root - INFO - Applying Tensor Parallelism with size 2...
2025-11-18 22:24:32,886 - root - INFO - Applying Tensor Parallelism with size 2...
2025-11-18 22:24:32,916 - root - INFO - Applying Tensor Parallelism with size 2...
2025-11-18 22:24:33,189 - root - INFO - Applying Tensor Parallelism with size 2...
2025-11-18 22:24:35,374 - root - INFO - Starting training!
2025-11-18 22:24:35,375 - root - INFO - Starting training!
2025-11-18 22:24:35,375 - root - INFO - Starting training!
2025-11-18 22:24:35,375 - root - INFO - Starting training!
2025-11-18 22:24:37,774 - root - INFO - Starting training!
2025-11-18 22:24:37,774 - root - INFO - Starting training!
2025-11-18 22:24:37,775 - root - INFO - Starting training!
2025-11-18 22:24:37,776 - root - INFO - Starting training!
[Rank 0] loss before all_reduce: DTensor(local_tensor=11.95706558227539, device_mesh=DeviceMesh('cuda', [0, 1], mesh_dim_names=('tensor',)), placements=(Replicate(),))
[Rank 2] loss before all_reduce: DTensor(local_tensor=11.954801559448242, device_mesh=DeviceMesh('cuda', [2, 3], mesh_dim_names=('tensor',)), placements=(Replicate(),))
[Rank 6] loss before all_reduce: DTensor(local_tensor=11.951972961425781, device_mesh=DeviceMesh('cuda', [6, 7], mesh_dim_names=('tensor',)), placements=(Replicate(),))
[Rank 4] loss before all_reduce: DTensor(local_tensor=11.961172103881836, device_mesh=DeviceMesh('cuda', [4, 5], mesh_dim_names=('tensor',)), placements=(Replicate(),))
[Rank 2] loss before all_reduce: torch.Size([])[Rank 0] loss before all_reduce: torch.Size([])

[Rank 6] loss before all_reduce: torch.Size([])
[Rank 4] loss before all_reduce: torch.Size([])
[Rank 3] loss before all_reduce: DTensor(local_tensor=11.954801559448242, device_mesh=DeviceMesh('cuda', [2, 3], mesh_dim_names=('tensor',)), placements=(Replicate(),))
[Rank 1] loss before all_reduce: DTensor(local_tensor=11.95706558227539, device_mesh=DeviceMesh('cuda', [0, 1], mesh_dim_names=('tensor',)), placements=(Replicate(),))
[Rank 7] loss before all_reduce: DTensor(local_tensor=11.951972961425781, device_mesh=DeviceMesh('cuda', [6, 7], mesh_dim_names=('tensor',)), placements=(Replicate(),))[Rank 5] loss before all_reduce: DTensor(local_tensor=11.961172103881836, device_mesh=DeviceMesh('cuda', [4, 5], mesh_dim_names=('tensor',)), placements=(Replicate(),))

[Rank 3] loss before all_reduce: torch.Size([])
[Rank 1] loss before all_reduce: torch.Size([])
[Rank 5] loss before all_reduce: torch.Size([])[Rank 7] loss before all_reduce: torch.Size([])

[rank3]: Traceback (most recent call last):
[rank3]:   File "/iopsstor/scratch/cscs/yiswang/large-sc/project/train.py", line 175, in <module>
[rank3]:     train(args)
[rank3]:   File "/iopsstor/scratch/cscs/yiswang/large-sc/project/train.py", line 150, in train
[rank3]:     loss.backward()
[rank3]:   File "/usr/local/lib/python3.12/dist-packages/torch/_tensor.py", line 626, in backward
[rank3]:     torch.autograd.backward(
[rank3]:   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py", line 347, in backward
[rank3]:     _engine_run_backward(
[rank3]:   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
[rank3]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]: RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.
[rank7]: Traceback (most recent call last):
[rank7]:   File "/iopsstor/scratch/cscs/yiswang/large-sc/project/train.py", line 175, in <module>
[rank7]:     train(args)
[rank7]:   File "/iopsstor/scratch/cscs/yiswang/large-sc/project/train.py", line 150, in train
[rank7]:     loss.backward()
[rank7]:   File "/usr/local/lib/python3.12/dist-packages/torch/_tensor.py", line 626, in backward
[rank7]:     torch.autograd.backward(
[rank7]:   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py", line 347, in backward
[rank7]:     _engine_run_backward(
[rank7]:   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
[rank7]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]: RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.
[rank5]: Traceback (most recent call last):
[rank5]:   File "/iopsstor/scratch/cscs/yiswang/large-sc/project/train.py", line 175, in <module>
[rank5]:     train(args)
[rank5]:   File "/iopsstor/scratch/cscs/yiswang/large-sc/project/train.py", line 150, in train
[rank5]:     loss.backward()
[rank5]:   File "/usr/local/lib/python3.12/dist-packages/torch/_tensor.py", line 626, in backward
[rank5]:     torch.autograd.backward(
[rank5]:   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py", line 347, in backward
[rank5]:     _engine_run_backward(
[rank5]:   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
[rank5]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]: RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.
[rank6]: Traceback (most recent call last):
[rank6]:   File "/iopsstor/scratch/cscs/yiswang/large-sc/project/train.py", line 175, in <module>
[rank6]:     train(args)
[rank6]:   File "/iopsstor/scratch/cscs/yiswang/large-sc/project/train.py", line 150, in train
[rank6]:     loss.backward()
[rank6]:   File "/usr/local/lib/python3.12/dist-packages/torch/_tensor.py", line 626, in backward
[rank6]:     torch.autograd.backward(
[rank6]:   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py", line 347, in backward
[rank6]:     _engine_run_backward(
[rank6]:   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
[rank6]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]: RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.
[rank4]: Traceback (most recent call last):
[rank4]:   File "/iopsstor/scratch/cscs/yiswang/large-sc/project/train.py", line 175, in <module>
[rank4]:     train(args)
[rank4]:   File "/iopsstor/scratch/cscs/yiswang/large-sc/project/train.py", line 150, in train
[rank4]:     loss.backward()
[rank4]:   File "/usr/local/lib/python3.12/dist-packages/torch/_tensor.py", line 626, in backward
[rank4]:     torch.autograd.backward(
[rank4]:   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py", line 347, in backward
[rank4]:     _engine_run_backward(
[rank4]:   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
[rank4]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]: RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.
[rank0]: Traceback (most recent call last):
[rank0]:   File "/iopsstor/scratch/cscs/yiswang/large-sc/project/train.py", line 175, in <module>
[rank0]:     train(args)
[rank0]:   File "/iopsstor/scratch/cscs/yiswang/large-sc/project/train.py", line 150, in train
[rank0]:     loss.backward()
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_tensor.py", line 626, in backward
[rank0]:     torch.autograd.backward(
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py", line 347, in backward
[rank0]:     _engine_run_backward(
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
[rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.
[rank2]: Traceback (most recent call last):
[rank2]:   File "/iopsstor/scratch/cscs/yiswang/large-sc/project/train.py", line 175, in <module>
[rank2]:     train(args)
[rank2]:   File "/iopsstor/scratch/cscs/yiswang/large-sc/project/train.py", line 150, in train
[rank2]:     loss.backward()
[rank2]:   File "/usr/local/lib/python3.12/dist-packages/torch/_tensor.py", line 626, in backward
[rank2]:     torch.autograd.backward(
[rank2]:   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py", line 347, in backward
[rank2]:     _engine_run_backward(
[rank2]:   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
[rank2]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]: RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.
[rank1]: Traceback (most recent call last):
[rank1]:   File "/iopsstor/scratch/cscs/yiswang/large-sc/project/train.py", line 175, in <module>
[rank1]:     train(args)
[rank1]:   File "/iopsstor/scratch/cscs/yiswang/large-sc/project/train.py", line 150, in train
[rank1]:     loss.backward()
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_tensor.py", line 626, in backward
[rank1]:     torch.autograd.backward(
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py", line 347, in backward
[rank1]:     _engine_run_backward(
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
[rank1]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]: RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.
[rank6]:[W1118 22:24:42.694715774 ProcessGroupNCCL.cpp:1427] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank4]:[W1118 22:24:42.808375091 ProcessGroupNCCL.cpp:1427] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank2]:[W1118 22:24:42.656152523 ProcessGroupNCCL.cpp:1427] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank0]:[W1118 22:24:42.670429741 ProcessGroupNCCL.cpp:1427] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W1118 22:24:43.056000 107200 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 107656 closing signal SIGTERM
W1118 22:24:43.057000 107200 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 107657 closing signal SIGTERM
W1118 22:24:43.058000 107200 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 107658 closing signal SIGTERM
W1118 22:24:43.151000 138041 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 138517 closing signal SIGTERM
W1118 22:24:43.153000 138041 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 138518 closing signal SIGTERM
W1118 22:24:43.154000 138041 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 138519 closing signal SIGTERM
E1118 22:24:43.474000 107200 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 3 (pid: 107659) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.6.0a0+ecf3bae40a.nv25.1', 'console_scripts', 'torchrun')())
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 918, in main
    run(args)
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/iopsstor/scratch/cscs/yiswang/large-sc/project/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-11-18_22:24:43
  host      : nid006698
  rank      : 7 (local_rank: 3)
  exitcode  : 1 (pid: 107659)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
E1118 22:24:43.734000 138041 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 3 (pid: 138520) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.6.0a0+ecf3bae40a.nv25.1', 'console_scripts', 'torchrun')())
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 918, in main
    run(args)
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/iopsstor/scratch/cscs/yiswang/large-sc/project/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-11-18_22:24:43
  host      : nid006665
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 138520)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: nid006698: task 1: Exited with exit code 1
srun: Terminating StepId=1107790.0
srun: error: nid006665: task 0: Exited with exit code 1
