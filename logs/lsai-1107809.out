START TIME: Tue Nov 18 22:33:27 CET 2025
END TIME: Tue Nov 18 22:33:27 CET 2025
[sbatch-master] running on nid006641
[sbatch-master] SLURM_NODELIST: nid[006641,007647]
[sbatch-master] SLURM_NNODES: 2
[sbatch-master] SLURM_NODEID: 0
[sbatch-master] define some env vars that will be passed to the compute nodes
[sbatch-master] execute command on compute nodes
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
[srun] rank=0 host=nid006641 noderank=0 localrank=0
W1118 22:33:46.024000 229382 torch/distributed/run.py:792] 
W1118 22:33:46.024000 229382 torch/distributed/run.py:792] *****************************************
W1118 22:33:46.024000 229382 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1118 22:33:46.024000 229382 torch/distributed/run.py:792] *****************************************
[srun] rank=1 host=nid007647 noderank=1 localrank=0
W1118 22:33:55.011000 133862 torch/distributed/run.py:792] 
W1118 22:33:55.011000 133862 torch/distributed/run.py:792] *****************************************
W1118 22:33:55.011000 133862 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1118 22:33:55.011000 133862 torch/distributed/run.py:792] *****************************************
2025-11-18 22:34:03,257 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=4096, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=5, training_steps=100, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False)
2025-11-18 22:34:03,257 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=4096, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=5, training_steps=100, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False)
2025-11-18 22:34:03,257 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=4096, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=5, training_steps=100, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False)
2025-11-18 22:34:03,257 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=4096, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=5, training_steps=100, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False)
2025-11-18 22:34:03,257 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=4096, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=5, training_steps=100, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False)
2025-11-18 22:34:03,257 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=4096, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=5, training_steps=100, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False)
2025-11-18 22:34:03,257 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=4096, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=5, training_steps=100, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False)
2025-11-18 22:34:03,258 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=4096, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=5, training_steps=100, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False)
2025-11-18 22:34:09,886 - root - INFO - Setting up DataLoaders...
2025-11-18 22:34:10,072 - root - INFO - Setting up DataLoaders...
2025-11-18 22:34:10,250 - root - INFO - Setting up DataLoaders...
2025-11-18 22:34:10,257 - root - INFO - Setting up DataLoaders...
2025-11-18 22:34:10,271 - root - INFO - Setting up DataLoaders...
2025-11-18 22:34:10,427 - root - INFO - Setting up DataLoaders...
2025-11-18 22:34:10,427 - root - INFO - Setting up DataLoaders...
2025-11-18 22:34:10,432 - root - INFO - Setting up DataLoaders...
2025-11-18 22:34:19,581 - root - INFO - Setting up Model...
2025-11-18 22:34:19,581 - root - INFO - Setting up Model...
2025-11-18 22:34:19,581 - root - INFO - Setting up Model...
2025-11-18 22:34:19,581 - root - INFO - Setting up Model...
2025-11-18 22:34:19,613 - root - INFO - Setting up Model...
2025-11-18 22:34:19,613 - root - INFO - Setting up Model...
2025-11-18 22:34:19,614 - root - INFO - Setting up Model...
2025-11-18 22:34:19,614 - root - INFO - Setting up Model...
2025-11-18 22:34:25,957 - root - INFO - Applying Tensor Parallelism with size 2...
2025-11-18 22:34:26,018 - root - INFO - Applying Tensor Parallelism with size 2...
2025-11-18 22:34:26,028 - root - INFO - Applying Tensor Parallelism with size 2...
2025-11-18 22:34:26,035 - root - INFO - Applying Tensor Parallelism with size 2...
2025-11-18 22:34:26,062 - root - INFO - Applying Tensor Parallelism with size 2...
2025-11-18 22:34:26,136 - root - INFO - Applying Tensor Parallelism with size 2...
2025-11-18 22:34:26,252 - root - INFO - Applying Tensor Parallelism with size 2...
2025-11-18 22:34:26,385 - root - INFO - Applying Tensor Parallelism with size 2...
2025-11-18 22:34:30,887 - root - INFO - Starting training!
2025-11-18 22:34:30,887 - root - INFO - Starting training!
2025-11-18 22:34:30,889 - root - INFO - Starting training!
2025-11-18 22:34:30,889 - root - INFO - Starting training!
2025-11-18 22:34:30,903 - root - INFO - Starting training!
2025-11-18 22:34:30,904 - root - INFO - Starting training!
2025-11-18 22:34:30,904 - root - INFO - Starting training!
2025-11-18 22:34:30,905 - root - INFO - Starting training!
2025-11-18 22:34:36,153 - root - INFO - Step: 1 | Loss (Avg): 11.98 | Tokens per second: 6242.64 | Training tokens per second (%): 19.55 | MFU (%): 3.37 | TFLOPs: 33.31
2025-11-18 22:34:38,469 - root - INFO - Step: 5 | Loss (Avg): 11.51 | Tokens per second: 56594.24 | Training tokens per second (%): 26.90 | MFU (%): 30.53 | TFLOPs: 301.95
2025-11-18 22:34:41,299 - root - INFO - Step: 10 | Loss (Avg): 10.07 | Tokens per second: 57909.17 | Training tokens per second (%): 24.17 | MFU (%): 31.24 | TFLOPs: 308.97
2025-11-18 22:34:44,419 - root - INFO - Step: 15 | Loss (Avg): 9.44 | Tokens per second: 52515.00 | Training tokens per second (%): 22.83 | MFU (%): 28.33 | TFLOPs: 280.19
2025-11-18 22:34:47,250 - root - INFO - Step: 20 | Loss (Avg): 9.05 | Tokens per second: 57881.70 | Training tokens per second (%): 24.43 | MFU (%): 31.23 | TFLOPs: 308.82
2025-11-18 22:34:50,064 - root - INFO - Step: 25 | Loss (Avg): 8.61 | Tokens per second: 58218.33 | Training tokens per second (%): 21.14 | MFU (%): 31.41 | TFLOPs: 310.62
2025-11-18 22:34:52,864 - root - INFO - Step: 30 | Loss (Avg): 8.27 | Tokens per second: 58534.23 | Training tokens per second (%): 20.92 | MFU (%): 31.58 | TFLOPs: 312.30
2025-11-18 22:34:55,750 - root - INFO - Step: 35 | Loss (Avg): 8.07 | Tokens per second: 56776.97 | Training tokens per second (%): 18.63 | MFU (%): 30.63 | TFLOPs: 302.93
2025-11-18 22:34:58,606 - root - INFO - Step: 40 | Loss (Avg): 7.75 | Tokens per second: 57371.00 | Training tokens per second (%): 18.74 | MFU (%): 30.95 | TFLOPs: 306.10
2025-11-18 22:35:01,650 - root - INFO - Step: 45 | Loss (Avg): 7.57 | Tokens per second: 53816.93 | Training tokens per second (%): 23.35 | MFU (%): 29.03 | TFLOPs: 287.13
2025-11-18 22:35:04,664 - root - INFO - Step: 50 | Loss (Avg): 7.44 | Tokens per second: 54371.52 | Training tokens per second (%): 19.41 | MFU (%): 29.33 | TFLOPs: 290.09
2025-11-18 22:35:07,512 - root - INFO - Step: 55 | Loss (Avg): 7.43 | Tokens per second: 57524.06 | Training tokens per second (%): 23.46 | MFU (%): 31.03 | TFLOPs: 306.91
2025-11-18 22:35:10,332 - root - INFO - Step: 60 | Loss (Avg): 7.38 | Tokens per second: 58105.77 | Training tokens per second (%): 24.94 | MFU (%): 31.35 | TFLOPs: 310.02
2025-11-18 22:35:13,161 - root - INFO - Step: 65 | Loss (Avg): 7.36 | Tokens per second: 57925.33 | Training tokens per second (%): 17.84 | MFU (%): 31.25 | TFLOPs: 309.05
2025-11-18 22:35:15,979 - root - INFO - Step: 70 | Loss (Avg): 7.14 | Tokens per second: 58144.41 | Training tokens per second (%): 18.75 | MFU (%): 31.37 | TFLOPs: 310.22
2025-11-18 22:35:18,792 - root - INFO - Step: 75 | Loss (Avg): 7.07 | Tokens per second: 58239.70 | Training tokens per second (%): 17.40 | MFU (%): 31.42 | TFLOPs: 310.73
2025-11-18 22:35:21,627 - root - INFO - Step: 80 | Loss (Avg): 7.31 | Tokens per second: 57813.22 | Training tokens per second (%): 24.71 | MFU (%): 31.19 | TFLOPs: 308.45
2025-11-18 22:35:24,459 - root - INFO - Step: 85 | Loss (Avg): 7.13 | Tokens per second: 57852.67 | Training tokens per second (%): 25.28 | MFU (%): 31.21 | TFLOPs: 308.67
2025-11-18 22:35:27,275 - root - INFO - Step: 90 | Loss (Avg): 7.07 | Tokens per second: 58192.35 | Training tokens per second (%): 14.79 | MFU (%): 31.39 | TFLOPs: 310.48
2025-11-18 22:35:30,119 - root - INFO - Step: 95 | Loss (Avg): 7.07 | Tokens per second: 57606.92 | Training tokens per second (%): 20.99 | MFU (%): 31.08 | TFLOPs: 307.35
2025-11-18 22:35:32,961 - root - INFO - Training completed
2025-11-18 22:35:32,961 - root - INFO - Step: 100 | Loss (Avg): 7.03 | Tokens per second: 57648.42 | Training tokens per second (%): 25.99 | MFU (%): 31.10 | TFLOPs: 307.58
2025-11-18 22:35:32,961 - root - INFO - Training completed
2025-11-18 22:35:32,961 - root - INFO - Training completed
2025-11-18 22:35:32,961 - root - INFO - Training completed
2025-11-18 22:35:32,961 - root - INFO - Training completed
2025-11-18 22:35:32,962 - root - INFO - Training completed
2025-11-18 22:35:32,963 - root - INFO - Training completed
2025-11-18 22:35:32,964 - root - INFO - Training completed
[sbatch-master] task finished
