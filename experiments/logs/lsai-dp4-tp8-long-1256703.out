START TIME: Thu Dec 18 00:48:49 CET 2025
[sbatch-master] running on nid007148
[sbatch-master] SLURM_NODELIST: nid[007148,007159,007166-007168,007174,007176-007177]
[sbatch-master] SLURM_NNODES: 8
[sbatch-master] SLURM_NODEID: 0
[sbatch-master] define some env vars that will be passed to the compute nodes
[sbatch-master] execute command on compute nodes
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
[srun] rank=0 host=nid007148 noderank=0 localrank=0
[srun] rank=1 host=nid007159 noderank=1 localrank=0
[srun] rank=3 host=nid007167 noderank=3 localrank=0
[srun] rank=2 host=nid007166 noderank=2 localrank=0
[srun] rank=6 host=nid007176 noderank=6 localrank=0
[srun] rank=4 host=nid007168 noderank=4 localrank=0
[srun] rank=7 host=nid007177 noderank=7 localrank=0
[srun] rank=5 host=nid007174 noderank=5 localrank=0
W1218 00:49:00.614000 20603 torch/distributed/run.py:792] 
W1218 00:49:00.614000 20603 torch/distributed/run.py:792] *****************************************
W1218 00:49:00.614000 20603 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1218 00:49:00.614000 20603 torch/distributed/run.py:792] *****************************************
W1218 00:49:01.104000 180087 torch/distributed/run.py:792] 
W1218 00:49:01.104000 180087 torch/distributed/run.py:792] *****************************************
W1218 00:49:01.104000 180087 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1218 00:49:01.104000 180087 torch/distributed/run.py:792] *****************************************
W1218 00:49:01.241000 51393 torch/distributed/run.py:792] 
W1218 00:49:01.241000 51393 torch/distributed/run.py:792] *****************************************
W1218 00:49:01.241000 51393 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1218 00:49:01.241000 51393 torch/distributed/run.py:792] *****************************************
W1218 00:49:01.294000 33115 torch/distributed/run.py:792] 
W1218 00:49:01.294000 33115 torch/distributed/run.py:792] *****************************************
W1218 00:49:01.294000 33115 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1218 00:49:01.294000 33115 torch/distributed/run.py:792] *****************************************
W1218 00:49:01.587000 262019 torch/distributed/run.py:792] 
W1218 00:49:01.587000 262019 torch/distributed/run.py:792] *****************************************
W1218 00:49:01.587000 262019 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1218 00:49:01.587000 262019 torch/distributed/run.py:792] *****************************************
W1218 00:49:01.680000 184028 torch/distributed/run.py:792] 
W1218 00:49:01.680000 184028 torch/distributed/run.py:792] *****************************************
W1218 00:49:01.680000 184028 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1218 00:49:01.680000 184028 torch/distributed/run.py:792] *****************************************
W1218 00:49:02.151000 226564 torch/distributed/run.py:792] 
W1218 00:49:02.151000 226564 torch/distributed/run.py:792] *****************************************
W1218 00:49:02.151000 226564 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1218 00:49:02.151000 226564 torch/distributed/run.py:792] *****************************************
W1218 00:49:02.732000 18402 torch/distributed/run.py:792] 
W1218 00:49:02.732000 18402 torch/distributed/run.py:792] *****************************************
W1218 00:49:02.732000 18402 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1218 00:49:02.732000 18402 torch/distributed/run.py:792] *****************************************
2025-12-18 00:49:08,016 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=5, checkpoint_frequency=100, checkpoint_save_path=None, checkpoint_load_path=None, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False, tp_size=8)
2025-12-18 00:49:08,016 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=5, checkpoint_frequency=100, checkpoint_save_path=None, checkpoint_load_path=None, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False, tp_size=8)
Setting device to local rank: 1Setting device to local rank: 3

2025-12-18 00:49:08,016 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=5, checkpoint_frequency=100, checkpoint_save_path=None, checkpoint_load_path=None, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False, tp_size=8)
Setting device to local rank: 2
2025-12-18 00:49:08,017 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=5, checkpoint_frequency=100, checkpoint_save_path=None, checkpoint_load_path=None, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False, tp_size=8)
Setting device to local rank: 0
2025-12-18 00:49:08,061 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=5, checkpoint_frequency=100, checkpoint_save_path=None, checkpoint_load_path=None, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False, tp_size=8)
Setting device to local rank: 0
2025-12-18 00:49:08,074 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=5, checkpoint_frequency=100, checkpoint_save_path=None, checkpoint_load_path=None, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False, tp_size=8)
Setting device to local rank: 1
2025-12-18 00:49:08,074 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=5, checkpoint_frequency=100, checkpoint_save_path=None, checkpoint_load_path=None, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False, tp_size=8)
Setting device to local rank: 3
2025-12-18 00:49:08,074 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=5, checkpoint_frequency=100, checkpoint_save_path=None, checkpoint_load_path=None, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False, tp_size=8)
Setting device to local rank: 2
2025-12-18 00:49:08,130 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=5, checkpoint_frequency=100, checkpoint_save_path=None, checkpoint_load_path=None, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False, tp_size=8)
2025-12-18 00:49:08,130 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=5, checkpoint_frequency=100, checkpoint_save_path=None, checkpoint_load_path=None, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False, tp_size=8)
Setting device to local rank: 0Setting device to local rank: 1

2025-12-18 00:49:08,130 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=5, checkpoint_frequency=100, checkpoint_save_path=None, checkpoint_load_path=None, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False, tp_size=8)
Setting device to local rank: 2
2025-12-18 00:49:08,130 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=5, checkpoint_frequency=100, checkpoint_save_path=None, checkpoint_load_path=None, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False, tp_size=8)
Setting device to local rank: 3
2025-12-18 00:49:08,432 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=5, checkpoint_frequency=100, checkpoint_save_path=None, checkpoint_load_path=None, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False, tp_size=8)
Setting device to local rank: 0
2025-12-18 00:49:08,433 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=5, checkpoint_frequency=100, checkpoint_save_path=None, checkpoint_load_path=None, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False, tp_size=8)
Setting device to local rank: 3
Setting device to local rank: 1
2025-12-18 00:49:08,433 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=5, checkpoint_frequency=100, checkpoint_save_path=None, checkpoint_load_path=None, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False, tp_size=8)
2025-12-18 00:49:08,507 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=5, checkpoint_frequency=100, checkpoint_save_path=None, checkpoint_load_path=None, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False, tp_size=8)
Setting device to local rank: 2
2025-12-18 00:49:08,577 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=5, checkpoint_frequency=100, checkpoint_save_path=None, checkpoint_load_path=None, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False, tp_size=8)
Setting device to local rank: 0
2025-12-18 00:49:08,577 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=5, checkpoint_frequency=100, checkpoint_save_path=None, checkpoint_load_path=None, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False, tp_size=8)
Setting device to local rank: 1
2025-12-18 00:49:08,577 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=5, checkpoint_frequency=100, checkpoint_save_path=None, checkpoint_load_path=None, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False, tp_size=8)
Setting device to local rank: 3
2025-12-18 00:49:08,577 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=5, checkpoint_frequency=100, checkpoint_save_path=None, checkpoint_load_path=None, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False, tp_size=8)
Setting device to local rank: 2
2025-12-18 00:49:09,169 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=5, checkpoint_frequency=100, checkpoint_save_path=None, checkpoint_load_path=None, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False, tp_size=8)
2025-12-18 00:49:09,169 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=5, checkpoint_frequency=100, checkpoint_save_path=None, checkpoint_load_path=None, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False, tp_size=8)
Setting device to local rank: 1Setting device to local rank: 3

Setting device to local rank: 2
Setting device to local rank: 0
2025-12-18 00:49:09,169 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=5, checkpoint_frequency=100, checkpoint_save_path=None, checkpoint_load_path=None, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False, tp_size=8)
2025-12-18 00:49:09,169 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=5, checkpoint_frequency=100, checkpoint_save_path=None, checkpoint_load_path=None, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False, tp_size=8)
2025-12-18 00:49:09,627 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=5, checkpoint_frequency=100, checkpoint_save_path=None, checkpoint_load_path=None, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False, tp_size=8)
Setting device to local rank: 2
Setting device to local rank: 1
Setting device to local rank: 0
2025-12-18 00:49:09,627 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=5, checkpoint_frequency=100, checkpoint_save_path=None, checkpoint_load_path=None, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False, tp_size=8)
2025-12-18 00:49:09,627 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=5, checkpoint_frequency=100, checkpoint_save_path=None, checkpoint_load_path=None, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False, tp_size=8)
2025-12-18 00:49:09,628 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=5, checkpoint_frequency=100, checkpoint_save_path=None, checkpoint_load_path=None, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False, tp_size=8)
Setting device to local rank: 3
2025-12-18 00:49:09,908 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=5, checkpoint_frequency=100, checkpoint_save_path=None, checkpoint_load_path=None, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False, tp_size=8)
2025-12-18 00:49:09,908 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=5, checkpoint_frequency=100, checkpoint_save_path=None, checkpoint_load_path=None, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False, tp_size=8)
2025-12-18 00:49:09,908 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=5, checkpoint_frequency=100, checkpoint_save_path=None, checkpoint_load_path=None, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False, tp_size=8)
Setting device to local rank: 1Setting device to local rank: 0Setting device to local rank: 2


2025-12-18 00:49:09,910 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=5, checkpoint_frequency=100, checkpoint_save_path=None, checkpoint_load_path=None, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False, tp_size=8)
Setting device to local rank: 3
[Rank 28] World Size: 32, DP: 3 / 4, TP: 4 / 8
2025-12-18 00:49:14,759 - root - INFO - Setting up DataLoaders...
[Rank 4] World Size: 32, DP: 0 / 4, TP: 4 / 8
2025-12-18 00:49:14,800 - root - INFO - Setting up DataLoaders...
[Rank 16] World Size: 32, DP: 2 / 4, TP: 0 / 8
2025-12-18 00:49:14,823 - root - INFO - Setting up DataLoaders...
[Rank 24] World Size: 32, DP: 3 / 4, TP: 0 / 8
2025-12-18 00:49:15,231 - root - INFO - Setting up DataLoaders...
[Rank 6] World Size: 32, DP: 0 / 4, TP: 6 / 8
2025-12-18 00:49:15,304 - root - INFO - Setting up DataLoaders...
[Rank 7] World Size: 32, DP: 0 / 4, TP: 7 / 8
2025-12-18 00:49:15,323 - root - INFO - Setting up DataLoaders...
[Rank 5] World Size: 32, DP: 0 / 4, TP: 5 / 8
2025-12-18 00:49:15,333 - root - INFO - Setting up DataLoaders...
[Rank 29] World Size: 32, DP: 3 / 4, TP: 5 / 8
2025-12-18 00:49:15,335 - root - INFO - Setting up DataLoaders...
[Rank 31] World Size: 32, DP: 3 / 4, TP: 7 / 8
2025-12-18 00:49:15,375 - root - INFO - Setting up DataLoaders...
[Rank 30] World Size: 32, DP: 3 / 4, TP: 6 / 8
2025-12-18 00:49:15,394 - root - INFO - Setting up DataLoaders...
[Rank 12] World Size: 32, DP: 1 / 4, TP: 4 / 8
2025-12-18 00:49:15,400 - root - INFO - Setting up DataLoaders...
[Rank 19] World Size: 32, DP: 2 / 4, TP: 3 / 8
[Rank 17] World Size: 32, DP: 2 / 4, TP: 1 / 8
2025-12-18 00:49:15,472 - root - INFO - Setting up DataLoaders...
2025-12-18 00:49:15,472 - root - INFO - Setting up DataLoaders...
[Rank 18] World Size: 32, DP: 2 / 4, TP: 2 / 8
2025-12-18 00:49:15,481 - root - INFO - Setting up DataLoaders...
[Rank 25] World Size: 32, DP: 3 / 4, TP: 1 / 8
2025-12-18 00:49:15,673 - root - INFO - Setting up DataLoaders...
[Rank 27] World Size: 32, DP: 3 / 4, TP: 3 / 8
2025-12-18 00:49:15,692 - root - INFO - Setting up DataLoaders...
[Rank 26] World Size: 32, DP: 3 / 4, TP: 2 / 8
2025-12-18 00:49:15,753 - root - INFO - Setting up DataLoaders...
[Rank 8] World Size: 32, DP: 1 / 4, TP: 0 / 8
2025-12-18 00:49:15,857 - root - INFO - Setting up DataLoaders...
[Rank 15] World Size: 32, DP: 1 / 4, TP: 7 / 8[Rank 14] World Size: 32, DP: 1 / 4, TP: 6 / 8

[Rank 13] World Size: 32, DP: 1 / 4, TP: 5 / 8
2025-12-18 00:49:16,031 - root - INFO - Setting up DataLoaders...
2025-12-18 00:49:16,031 - root - INFO - Setting up DataLoaders...
2025-12-18 00:49:16,031 - root - INFO - Setting up DataLoaders...
[Rank 9] World Size: 32, DP: 1 / 4, TP: 1 / 8[Rank 11] World Size: 32, DP: 1 / 4, TP: 3 / 8

2025-12-18 00:49:16,420 - root - INFO - Setting up DataLoaders...
2025-12-18 00:49:16,420 - root - INFO - Setting up DataLoaders...
[Rank 10] World Size: 32, DP: 1 / 4, TP: 2 / 8
2025-12-18 00:49:16,429 - root - INFO - Setting up DataLoaders...
[Rank 0] World Size: 32, DP: 0 / 4, TP: 0 / 8
2025-12-18 00:49:16,490 - root - INFO - Setting up DataLoaders...
[Rank 20] World Size: 32, DP: 2 / 4, TP: 4 / 8
2025-12-18 00:49:16,939 - root - INFO - Setting up DataLoaders...
[Rank 2] World Size: 32, DP: 0 / 4, TP: 2 / 8
2025-12-18 00:49:16,995 - root - INFO - Setting up DataLoaders...
[Rank 3] World Size: 32, DP: 0 / 4, TP: 3 / 8
2025-12-18 00:49:17,055 - root - INFO - Setting up DataLoaders...
[Rank 1] World Size: 32, DP: 0 / 4, TP: 1 / 8
2025-12-18 00:49:17,055 - root - INFO - Setting up DataLoaders...
[Rank 22] World Size: 32, DP: 2 / 4, TP: 6 / 8
2025-12-18 00:49:17,578 - root - INFO - Setting up DataLoaders...
[Rank 23] World Size: 32, DP: 2 / 4, TP: 7 / 8
2025-12-18 00:49:17,579 - root - INFO - Setting up DataLoaders...
[Rank 21] World Size: 32, DP: 2 / 4, TP: 5 / 8
2025-12-18 00:49:17,587 - root - INFO - Setting up DataLoaders...
2025-12-18 00:49:19,672 - root - INFO - Setting up Model...
2025-12-18 00:49:19,692 - root - INFO - Applying Tensor Parallelism with size 8...
2025-12-18 00:49:19,710 - root - INFO - Setting up Model...
2025-12-18 00:49:19,731 - root - INFO - Applying Tensor Parallelism with size 8...
2025-12-18 00:49:19,970 - root - INFO - Setting up Model...
2025-12-18 00:49:19,996 - root - INFO - Applying Tensor Parallelism with size 8...
2025-12-18 00:49:20,082 - root - INFO - Setting up Model...
2025-12-18 00:49:20,090 - root - INFO - Setting up Model...
2025-12-18 00:49:20,099 - root - INFO - Setting up Model...
2025-12-18 00:49:20,103 - root - INFO - Applying Tensor Parallelism with size 8...
2025-12-18 00:49:20,109 - root - INFO - Applying Tensor Parallelism with size 8...
2025-12-18 00:49:20,118 - root - INFO - Applying Tensor Parallelism with size 8...
2025-12-18 00:49:20,181 - root - INFO - Setting up Model...
2025-12-18 00:49:20,190 - root - INFO - Setting up Model...
2025-12-18 00:49:20,199 - root - INFO - Setting up Model...
2025-12-18 00:49:20,206 - root - INFO - Applying Tensor Parallelism with size 8...
2025-12-18 00:49:20,208 - root - INFO - Applying Tensor Parallelism with size 8...
2025-12-18 00:49:20,218 - root - INFO - Applying Tensor Parallelism with size 8...
2025-12-18 00:49:20,220 - root - INFO - Setting up Model...
2025-12-18 00:49:20,220 - root - INFO - Setting up Model...
2025-12-18 00:49:20,226 - root - INFO - Setting up Model...
2025-12-18 00:49:20,240 - root - INFO - Applying Tensor Parallelism with size 8...
2025-12-18 00:49:20,241 - root - INFO - Applying Tensor Parallelism with size 8...
2025-12-18 00:49:20,246 - root - INFO - Applying Tensor Parallelism with size 8...
2025-12-18 00:49:20,250 - root - INFO - Setting up Model...
2025-12-18 00:49:20,269 - root - INFO - Applying Tensor Parallelism with size 8...
[rank16]:[W1218 00:49:20.596556166 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 16]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-12-18 00:49:20,289 - root - INFO - Setting up Model...
2025-12-18 00:49:20,310 - root - INFO - Applying Tensor Parallelism with size 8...
[rank4]:[W1218 00:49:20.755732230 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 4]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-12-18 00:49:20,429 - root - INFO - Setting up Model...
[rank6]:[W1218 00:49:20.815690649 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 6]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-12-18 00:49:20,448 - root - INFO - Applying Tensor Parallelism with size 8...
[rank5]:[W1218 00:49:20.832662393 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 5]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-12-18 00:49:20,488 - root - INFO - Setting up Model...
2025-12-18 00:49:20,506 - root - INFO - Setting up Model...
2025-12-18 00:49:20,510 - root - INFO - Applying Tensor Parallelism with size 8...
[rank7]:[W1218 00:49:20.895435660 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 7]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-12-18 00:49:20,533 - root - INFO - Applying Tensor Parallelism with size 8...
[rank31]:[W1218 00:49:20.852368712 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 31]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank30]:[W1218 00:49:20.854084473 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 30]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank29]:[W1218 00:49:20.876059227 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 29]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank28]:[W1218 00:49:20.901805779 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 28]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank17]:[W1218 00:49:20.000693353 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 17]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank19]:[W1218 00:49:20.007151363 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 19]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank18]:[W1218 00:49:20.011391542 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 18]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank24]:[W1218 00:49:20.126549414 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 24]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank25]:[W1218 00:49:20.197436702 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 25]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-12-18 00:49:20,791 - root - INFO - Setting up Model...
2025-12-18 00:49:20,799 - root - INFO - Setting up Model...
2025-12-18 00:49:20,805 - root - INFO - Setting up Model...
2025-12-18 00:49:20,814 - root - INFO - Applying Tensor Parallelism with size 8...
2025-12-18 00:49:20,821 - root - INFO - Applying Tensor Parallelism with size 8...
2025-12-18 00:49:20,826 - root - INFO - Applying Tensor Parallelism with size 8...
[rank27]:[W1218 00:49:20.294851428 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 27]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-12-18 00:49:20,891 - root - INFO - Setting up Model...
2025-12-18 00:49:20,910 - root - INFO - Applying Tensor Parallelism with size 8...
[rank12]:[W1218 00:49:20.673822120 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 12]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank26]:[W1218 00:49:21.461230064 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 26]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank13]:[W1218 00:49:21.974564310 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 13]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank15]:[W1218 00:49:21.987256405 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 15]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank14]:[W1218 00:49:21.988812370 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 14]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-12-18 00:49:21,232 - root - INFO - Setting up Model...
2025-12-18 00:49:21,236 - root - INFO - Setting up Model...
2025-12-18 00:49:21,252 - root - INFO - Applying Tensor Parallelism with size 8...
2025-12-18 00:49:21,256 - root - INFO - Applying Tensor Parallelism with size 8...
2025-12-18 00:49:21,371 - root - INFO - Setting up Model...
2025-12-18 00:49:21,396 - root - INFO - Applying Tensor Parallelism with size 8...
2025-12-18 00:49:21,464 - root - INFO - Setting up Model...
2025-12-18 00:49:21,489 - root - INFO - Applying Tensor Parallelism with size 8...
[rank8]:[W1218 00:49:21.840319065 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 8]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank10]:[W1218 00:49:21.869903829 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 10]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank11]:[W1218 00:49:21.870749729 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 11]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank9]:[W1218 00:49:21.994529708 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 9]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-12-18 00:49:21,834 - root - INFO - Setting up Model...
2025-12-18 00:49:21,841 - root - INFO - Setting up Model...
2025-12-18 00:49:21,841 - root - INFO - Setting up Model...
2025-12-18 00:49:21,860 - root - INFO - Applying Tensor Parallelism with size 8...
2025-12-18 00:49:21,861 - root - INFO - Applying Tensor Parallelism with size 8...
2025-12-18 00:49:21,864 - root - INFO - Applying Tensor Parallelism with size 8...
2025-12-18 00:49:21,880 - root - INFO - Setting up Model...
2025-12-18 00:49:21,899 - root - INFO - Applying Tensor Parallelism with size 8...
[rank3]:[W1218 00:49:22.322939434 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank0]:[W1218 00:49:22.361688675 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank1]:[W1218 00:49:22.448443224 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank2]:[W1218 00:49:22.448466360 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-12-18 00:49:22,382 - root - INFO - Setting up Model...
2025-12-18 00:49:22,383 - root - INFO - Setting up Model...
2025-12-18 00:49:22,404 - root - INFO - Applying Tensor Parallelism with size 8...
2025-12-18 00:49:22,419 - root - INFO - Applying Tensor Parallelism with size 8...
2025-12-18 00:49:22,447 - root - INFO - Setting up Model...
2025-12-18 00:49:22,465 - root - INFO - Applying Tensor Parallelism with size 8...
[rank20]:[W1218 00:49:22.074325638 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 20]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank22]:[W1218 00:49:23.942565022 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 22]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank21]:[W1218 00:49:23.965254710 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 21]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank23]:[W1218 00:49:23.013950974 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 23]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-12-18 00:49:29,098 - root - INFO - Starting training!
2025-12-18 00:49:29,098 - root - INFO - Starting training!
2025-12-18 00:49:29,097 - root - INFO - Starting training!
2025-12-18 00:49:29,097 - root - INFO - Starting training!
2025-12-18 00:49:29,097 - root - INFO - Starting training!
2025-12-18 00:49:29,097 - root - INFO - Starting training!
2025-12-18 00:49:29,098 - root - INFO - Starting training!
2025-12-18 00:49:29,097 - root - INFO - Starting training!
2025-12-18 00:49:29,097 - root - INFO - Starting training!
2025-12-18 00:49:29,098 - root - INFO - Starting training!
2025-12-18 00:49:29,097 - root - INFO - Starting training!
2025-12-18 00:49:29,097 - root - INFO - Starting training!
2025-12-18 00:49:29,097 - root - INFO - Starting training!
2025-12-18 00:49:29,097 - root - INFO - Starting training!
2025-12-18 00:49:29,098 - root - INFO - Starting training!
2025-12-18 00:49:29,098 - root - INFO - Starting training!
2025-12-18 00:49:29,098 - root - INFO - Starting training!
2025-12-18 00:49:29,098 - root - INFO - Starting training!
2025-12-18 00:49:29,098 - root - INFO - Starting training!
2025-12-18 00:49:29,098 - root - INFO - Starting training!
2025-12-18 00:49:29,098 - root - INFO - Starting training!
2025-12-18 00:49:29,098 - root - INFO - Starting training!
2025-12-18 00:49:29,098 - root - INFO - Starting training!
2025-12-18 00:49:29,098 - root - INFO - Starting training!
2025-12-18 00:49:29,098 - root - INFO - Starting training!
2025-12-18 00:49:29,098 - root - INFO - Starting training!
2025-12-18 00:49:29,098 - root - INFO - Starting training!
2025-12-18 00:49:29,098 - root - INFO - Starting training!
2025-12-18 00:49:29,098 - root - INFO - Starting training!
2025-12-18 00:49:29,098 - root - INFO - Starting training!
2025-12-18 00:49:29,098 - root - INFO - Starting training!
2025-12-18 00:49:29,098 - root - INFO - Starting training!
2025-12-18 00:49:35,826 - root - INFO - Step: 1 | Loss (Avg): 11.96 | Reserved Memory 29.22 GB  | Tokens per second: 9739.90 | Training tokens per second (%): 10.75 | MFU (%): 1.49 | TFLOP/s/GPU: 14.71
2025-12-18 00:49:40,786 - root - INFO - Step: 5 | Loss (Avg): 11.94 | Reserved Memory 34.23 GB  | Tokens per second: 52870.29 | Training tokens per second (%): 10.38 | MFU (%): 8.07 | TFLOP/s/GPU: 79.83
2025-12-18 00:49:47,030 - root - INFO - Step: 10 | Loss (Avg): 11.85 | Reserved Memory 34.23 GB  | Tokens per second: 52487.50 | Training tokens per second (%): 9.39 | MFU (%): 8.01 | TFLOP/s/GPU: 79.26
2025-12-18 00:49:53,293 - root - INFO - Step: 15 | Loss (Avg): 11.66 | Reserved Memory 34.23 GB  | Tokens per second: 52333.63 | Training tokens per second (%): 9.98 | MFU (%): 7.99 | TFLOP/s/GPU: 79.02
2025-12-18 00:49:59,540 - root - INFO - Step: 20 | Loss (Avg): 11.30 | Reserved Memory 34.23 GB  | Tokens per second: 52461.14 | Training tokens per second (%): 9.30 | MFU (%): 8.01 | TFLOP/s/GPU: 79.22
2025-12-18 00:50:05,804 - root - INFO - Step: 25 | Loss (Avg): 10.92 | Reserved Memory 34.23 GB  | Tokens per second: 52318.06 | Training tokens per second (%): 10.43 | MFU (%): 7.99 | TFLOP/s/GPU: 79.00
2025-12-18 00:50:12,226 - root - INFO - Step: 30 | Loss (Avg): 10.60 | Reserved Memory 35.23 GB  | Tokens per second: 51038.99 | Training tokens per second (%): 9.06 | MFU (%): 7.79 | TFLOP/s/GPU: 77.07
2025-12-18 00:50:18,466 - root - INFO - Step: 35 | Loss (Avg): 10.27 | Reserved Memory 35.23 GB  | Tokens per second: 52517.88 | Training tokens per second (%): 9.55 | MFU (%): 8.02 | TFLOP/s/GPU: 79.30
2025-12-18 00:50:24,747 - root - INFO - Step: 40 | Loss (Avg): 10.06 | Reserved Memory 35.23 GB  | Tokens per second: 52183.53 | Training tokens per second (%): 10.63 | MFU (%): 7.97 | TFLOP/s/GPU: 78.80
2025-12-18 00:50:31,022 - root - INFO - Step: 45 | Loss (Avg): 9.46 | Reserved Memory 35.23 GB  | Tokens per second: 52230.75 | Training tokens per second (%): 11.82 | MFU (%): 7.97 | TFLOP/s/GPU: 78.87
2025-12-18 00:50:37,250 - root - INFO - Step: 50 | Loss (Avg): 9.05 | Reserved Memory 35.23 GB  | Tokens per second: 52622.15 | Training tokens per second (%): 10.59 | MFU (%): 8.03 | TFLOP/s/GPU: 79.46
2025-12-18 00:50:43,501 - root - INFO - Step: 55 | Loss (Avg): 8.50 | Reserved Memory 35.23 GB  | Tokens per second: 52429.83 | Training tokens per second (%): 7.77 | MFU (%): 8.00 | TFLOP/s/GPU: 79.17
2025-12-18 00:50:49,733 - root - INFO - Step: 60 | Loss (Avg): 8.09 | Reserved Memory 35.23 GB  | Tokens per second: 52589.10 | Training tokens per second (%): 8.12 | MFU (%): 8.03 | TFLOP/s/GPU: 79.41
2025-12-18 00:50:56,005 - root - INFO - Step: 65 | Loss (Avg): 7.84 | Reserved Memory 35.23 GB  | Tokens per second: 52257.38 | Training tokens per second (%): 9.88 | MFU (%): 7.98 | TFLOP/s/GPU: 78.91
2025-12-18 00:51:02,452 - root - INFO - Step: 70 | Loss (Avg): 7.57 | Reserved Memory 35.23 GB  | Tokens per second: 50838.71 | Training tokens per second (%): 9.63 | MFU (%): 7.76 | TFLOP/s/GPU: 76.77
2025-12-18 00:51:08,726 - root - INFO - Step: 75 | Loss (Avg): 7.47 | Reserved Memory 35.23 GB  | Tokens per second: 52234.90 | Training tokens per second (%): 7.23 | MFU (%): 7.98 | TFLOP/s/GPU: 78.87
2025-12-18 00:51:14,987 - root - INFO - Step: 80 | Loss (Avg): 7.43 | Reserved Memory 35.23 GB  | Tokens per second: 52342.50 | Training tokens per second (%): 10.44 | MFU (%): 7.99 | TFLOP/s/GPU: 79.04
2025-12-18 00:51:21,274 - root - INFO - Step: 85 | Loss (Avg): 7.37 | Reserved Memory 35.23 GB  | Tokens per second: 52132.32 | Training tokens per second (%): 8.59 | MFU (%): 7.96 | TFLOP/s/GPU: 78.72
2025-12-18 00:51:27,521 - root - INFO - Step: 90 | Loss (Avg): 7.31 | Reserved Memory 35.23 GB  | Tokens per second: 52458.06 | Training tokens per second (%): 9.28 | MFU (%): 8.01 | TFLOP/s/GPU: 79.21
2025-12-18 00:51:33,792 - root - INFO - Step: 95 | Loss (Avg): 7.15 | Reserved Memory 35.23 GB  | Tokens per second: 52264.27 | Training tokens per second (%): 9.51 | MFU (%): 7.98 | TFLOP/s/GPU: 78.92
2025-12-18 00:51:40,058 - root - INFO - Step: 100 | Loss (Avg): 7.24 | Reserved Memory 35.23 GB  | Tokens per second: 52302.68 | Training tokens per second (%): 10.73 | MFU (%): 7.99 | TFLOP/s/GPU: 78.98
2025-12-18 00:51:46,340 - root - INFO - Step: 105 | Loss (Avg): 7.18 | Reserved Memory 35.23 GB  | Tokens per second: 52176.73 | Training tokens per second (%): 10.17 | MFU (%): 7.97 | TFLOP/s/GPU: 78.79
2025-12-18 00:51:52,624 - root - INFO - Step: 110 | Loss (Avg): 7.15 | Reserved Memory 35.23 GB  | Tokens per second: 52152.86 | Training tokens per second (%): 7.27 | MFU (%): 7.96 | TFLOP/s/GPU: 78.75
2025-12-18 00:51:59,025 - root - INFO - Step: 115 | Loss (Avg): 7.13 | Reserved Memory 35.23 GB  | Tokens per second: 51198.76 | Training tokens per second (%): 8.01 | MFU (%): 7.82 | TFLOP/s/GPU: 77.31
2025-12-18 00:52:05,307 - root - INFO - Step: 120 | Loss (Avg): 7.17 | Reserved Memory 35.23 GB  | Tokens per second: 52169.85 | Training tokens per second (%): 10.63 | MFU (%): 7.97 | TFLOP/s/GPU: 78.78
2025-12-18 00:52:11,563 - root - INFO - Step: 125 | Loss (Avg): 6.95 | Reserved Memory 35.23 GB  | Tokens per second: 52388.21 | Training tokens per second (%): 10.92 | MFU (%): 8.00 | TFLOP/s/GPU: 79.11
2025-12-18 00:52:17,839 - root - INFO - Step: 130 | Loss (Avg): 7.00 | Reserved Memory 35.23 GB  | Tokens per second: 52218.40 | Training tokens per second (%): 10.02 | MFU (%): 7.97 | TFLOP/s/GPU: 78.85
2025-12-18 00:52:24,102 - root - INFO - Step: 135 | Loss (Avg): 7.08 | Reserved Memory 35.23 GB  | Tokens per second: 52332.86 | Training tokens per second (%): 8.93 | MFU (%): 7.99 | TFLOP/s/GPU: 79.02
2025-12-18 00:52:30,354 - root - INFO - Step: 140 | Loss (Avg): 6.90 | Reserved Memory 35.23 GB  | Tokens per second: 52420.02 | Training tokens per second (%): 8.09 | MFU (%): 8.00 | TFLOP/s/GPU: 79.15
2025-12-18 00:52:36,599 - root - INFO - Step: 145 | Loss (Avg): 7.06 | Reserved Memory 35.23 GB  | Tokens per second: 52483.96 | Training tokens per second (%): 10.49 | MFU (%): 8.01 | TFLOP/s/GPU: 79.25
2025-12-18 00:52:42,986 - root - INFO - Step: 150 | Loss (Avg): 6.87 | Reserved Memory 35.23 GB  | Tokens per second: 51308.47 | Training tokens per second (%): 11.10 | MFU (%): 7.83 | TFLOP/s/GPU: 77.48
2025-12-18 00:52:49,278 - root - INFO - Step: 155 | Loss (Avg): 6.92 | Reserved Memory 35.23 GB  | Tokens per second: 52097.13 | Training tokens per second (%): 9.67 | MFU (%): 7.95 | TFLOP/s/GPU: 78.67
2025-12-18 00:52:55,471 - root - INFO - Step: 160 | Loss (Avg): 6.87 | Reserved Memory 35.23 GB  | Tokens per second: 52915.20 | Training tokens per second (%): 8.93 | MFU (%): 8.08 | TFLOP/s/GPU: 79.90
2025-12-18 00:53:01,701 - root - INFO - Step: 165 | Loss (Avg): 6.78 | Reserved Memory 35.23 GB  | Tokens per second: 52608.90 | Training tokens per second (%): 7.96 | MFU (%): 8.03 | TFLOP/s/GPU: 79.44
2025-12-18 00:53:07,880 - root - INFO - Step: 170 | Loss (Avg): 6.80 | Reserved Memory 35.23 GB  | Tokens per second: 53043.44 | Training tokens per second (%): 10.32 | MFU (%): 8.10 | TFLOP/s/GPU: 80.10
2025-12-18 00:53:14,081 - root - INFO - Step: 175 | Loss (Avg): 6.66 | Reserved Memory 35.23 GB  | Tokens per second: 52855.76 | Training tokens per second (%): 9.36 | MFU (%): 8.07 | TFLOP/s/GPU: 79.81
2025-12-18 00:53:20,308 - root - INFO - Step: 180 | Loss (Avg): 6.73 | Reserved Memory 35.23 GB  | Tokens per second: 52632.40 | Training tokens per second (%): 8.36 | MFU (%): 8.04 | TFLOP/s/GPU: 79.47
2025-12-18 00:53:26,511 - root - INFO - Step: 185 | Loss (Avg): 6.80 | Reserved Memory 35.23 GB  | Tokens per second: 52836.64 | Training tokens per second (%): 8.79 | MFU (%): 8.07 | TFLOP/s/GPU: 79.78
2025-12-18 00:53:32,773 - root - INFO - Step: 190 | Loss (Avg): 6.76 | Reserved Memory 35.23 GB  | Tokens per second: 52346.79 | Training tokens per second (%): 9.51 | MFU (%): 7.99 | TFLOP/s/GPU: 79.04
2025-12-18 00:53:39,177 - root - INFO - Step: 195 | Loss (Avg): 6.72 | Reserved Memory 35.23 GB  | Tokens per second: 51176.61 | Training tokens per second (%): 10.21 | MFU (%): 7.81 | TFLOP/s/GPU: 77.28
2025-12-18 00:53:45,419 - root - INFO - Step: 200 | Loss (Avg): 6.74 | Reserved Memory 35.23 GB  | Tokens per second: 52503.86 | Training tokens per second (%): 10.33 | MFU (%): 8.02 | TFLOP/s/GPU: 79.28
2025-12-18 00:53:51,610 - root - INFO - Step: 205 | Loss (Avg): 6.78 | Reserved Memory 35.23 GB  | Tokens per second: 52940.06 | Training tokens per second (%): 8.59 | MFU (%): 8.08 | TFLOP/s/GPU: 79.94
2025-12-18 00:53:57,866 - root - INFO - Step: 210 | Loss (Avg): 6.77 | Reserved Memory 35.23 GB  | Tokens per second: 52389.01 | Training tokens per second (%): 8.64 | MFU (%): 8.00 | TFLOP/s/GPU: 79.11
2025-12-18 00:54:04,062 - root - INFO - Step: 215 | Loss (Avg): 6.59 | Reserved Memory 35.23 GB  | Tokens per second: 52896.39 | Training tokens per second (%): 9.03 | MFU (%): 8.08 | TFLOP/s/GPU: 79.87
2025-12-18 00:54:10,271 - root - INFO - Step: 220 | Loss (Avg): 6.78 | Reserved Memory 35.23 GB  | Tokens per second: 52791.80 | Training tokens per second (%): 10.35 | MFU (%): 8.06 | TFLOP/s/GPU: 79.72
2025-12-18 00:54:16,578 - root - INFO - Step: 225 | Loss (Avg): 6.75 | Reserved Memory 35.23 GB  | Tokens per second: 51964.17 | Training tokens per second (%): 9.79 | MFU (%): 7.93 | TFLOP/s/GPU: 78.47
2025-12-18 00:54:22,960 - root - INFO - Step: 230 | Loss (Avg): 6.62 | Reserved Memory 35.23 GB  | Tokens per second: 51355.31 | Training tokens per second (%): 7.84 | MFU (%): 7.84 | TFLOP/s/GPU: 77.55
2025-12-18 00:54:29,153 - root - INFO - Step: 235 | Loss (Avg): 6.58 | Reserved Memory 35.23 GB  | Tokens per second: 52917.09 | Training tokens per second (%): 10.14 | MFU (%): 8.08 | TFLOP/s/GPU: 79.90
2025-12-18 00:54:35,349 - root - INFO - Step: 240 | Loss (Avg): 6.64 | Reserved Memory 35.23 GB  | Tokens per second: 52893.94 | Training tokens per second (%): 10.71 | MFU (%): 8.08 | TFLOP/s/GPU: 79.87
2025-12-18 00:54:41,529 - root - INFO - Step: 245 | Loss (Avg): 6.54 | Reserved Memory 35.23 GB  | Tokens per second: 53039.99 | Training tokens per second (%): 9.97 | MFU (%): 8.10 | TFLOP/s/GPU: 80.09
2025-12-18 00:54:47,730 - root - INFO - Step: 250 | Loss (Avg): 6.56 | Reserved Memory 35.23 GB  | Tokens per second: 52842.54 | Training tokens per second (%): 10.37 | MFU (%): 8.07 | TFLOP/s/GPU: 79.79
2025-12-18 00:54:53,880 - root - INFO - Step: 255 | Loss (Avg): 6.76 | Reserved Memory 35.23 GB  | Tokens per second: 53298.24 | Training tokens per second (%): 8.52 | MFU (%): 8.14 | TFLOP/s/GPU: 80.48
2025-12-18 00:55:00,129 - root - INFO - Step: 260 | Loss (Avg): 6.61 | Reserved Memory 35.23 GB  | Tokens per second: 52444.83 | Training tokens per second (%): 8.67 | MFU (%): 8.01 | TFLOP/s/GPU: 79.19
2025-12-18 00:55:06,335 - root - INFO - Step: 265 | Loss (Avg): 6.59 | Reserved Memory 35.23 GB  | Tokens per second: 52809.17 | Training tokens per second (%): 6.80 | MFU (%): 8.06 | TFLOP/s/GPU: 79.74
2025-12-18 00:55:12,879 - root - INFO - Step: 270 | Loss (Avg): 6.61 | Reserved Memory 35.23 GB  | Tokens per second: 50085.26 | Training tokens per second (%): 9.36 | MFU (%): 7.65 | TFLOP/s/GPU: 75.63
2025-12-18 00:55:19,099 - root - INFO - Step: 275 | Loss (Avg): 6.46 | Reserved Memory 35.23 GB  | Tokens per second: 52691.14 | Training tokens per second (%): 7.75 | MFU (%): 8.04 | TFLOP/s/GPU: 79.56
2025-12-18 00:55:25,305 - root - INFO - Step: 280 | Loss (Avg): 6.57 | Reserved Memory 35.23 GB  | Tokens per second: 52814.58 | Training tokens per second (%): 8.31 | MFU (%): 8.06 | TFLOP/s/GPU: 79.75
2025-12-18 00:55:31,509 - root - INFO - Step: 285 | Loss (Avg): 6.56 | Reserved Memory 35.23 GB  | Tokens per second: 52833.01 | Training tokens per second (%): 9.95 | MFU (%): 8.07 | TFLOP/s/GPU: 79.78
2025-12-18 00:55:37,694 - root - INFO - Step: 290 | Loss (Avg): 6.52 | Reserved Memory 35.23 GB  | Tokens per second: 52994.96 | Training tokens per second (%): 9.58 | MFU (%): 8.09 | TFLOP/s/GPU: 80.02
2025-12-18 00:55:43,873 - root - INFO - Step: 295 | Loss (Avg): 6.53 | Reserved Memory 35.23 GB  | Tokens per second: 53043.06 | Training tokens per second (%): 9.48 | MFU (%): 8.10 | TFLOP/s/GPU: 80.09
2025-12-18 00:55:50,287 - root - INFO - Step: 300 | Loss (Avg): 6.41 | Reserved Memory 35.23 GB  | Tokens per second: 51096.92 | Training tokens per second (%): 8.70 | MFU (%): 7.80 | TFLOP/s/GPU: 77.16
2025-12-18 00:55:56,581 - root - INFO - Step: 305 | Loss (Avg): 6.64 | Reserved Memory 35.23 GB  | Tokens per second: 52071.90 | Training tokens per second (%): 8.93 | MFU (%): 7.95 | TFLOP/s/GPU: 78.63
2025-12-18 00:56:02,801 - root - INFO - Step: 310 | Loss (Avg): 6.58 | Reserved Memory 35.23 GB  | Tokens per second: 52690.23 | Training tokens per second (%): 10.56 | MFU (%): 8.04 | TFLOP/s/GPU: 79.56
2025-12-18 00:56:08,995 - root - INFO - Step: 315 | Loss (Avg): 6.51 | Reserved Memory 35.23 GB  | Tokens per second: 52917.64 | Training tokens per second (%): 10.24 | MFU (%): 8.08 | TFLOP/s/GPU: 79.91
2025-12-18 00:56:15,205 - root - INFO - Step: 320 | Loss (Avg): 6.48 | Reserved Memory 35.23 GB  | Tokens per second: 52776.26 | Training tokens per second (%): 11.82 | MFU (%): 8.06 | TFLOP/s/GPU: 79.69
2025-12-18 00:56:21,363 - root - INFO - Step: 325 | Loss (Avg): 6.61 | Reserved Memory 35.23 GB  | Tokens per second: 53222.33 | Training tokens per second (%): 12.06 | MFU (%): 8.13 | TFLOP/s/GPU: 80.37
2025-12-18 00:56:27,507 - root - INFO - Step: 330 | Loss (Avg): 6.53 | Reserved Memory 35.23 GB  | Tokens per second: 53348.29 | Training tokens per second (%): 9.54 | MFU (%): 8.15 | TFLOP/s/GPU: 80.56
2025-12-18 00:56:33,682 - root - INFO - Step: 335 | Loss (Avg): 6.54 | Reserved Memory 35.23 GB  | Tokens per second: 53078.24 | Training tokens per second (%): 10.30 | MFU (%): 8.10 | TFLOP/s/GPU: 80.15
2025-12-18 00:56:39,863 - root - INFO - Step: 340 | Loss (Avg): 6.54 | Reserved Memory 35.23 GB  | Tokens per second: 53017.63 | Training tokens per second (%): 8.54 | MFU (%): 8.09 | TFLOP/s/GPU: 80.06
2025-12-18 00:56:46,252 - root - INFO - Step: 345 | Loss (Avg): 6.40 | Reserved Memory 35.23 GB  | Tokens per second: 51298.72 | Training tokens per second (%): 8.68 | MFU (%): 7.83 | TFLOP/s/GPU: 77.46
2025-12-18 00:56:52,590 - root - INFO - Step: 350 | Loss (Avg): 6.55 | Reserved Memory 35.23 GB  | Tokens per second: 51711.95 | Training tokens per second (%): 11.22 | MFU (%): 7.90 | TFLOP/s/GPU: 78.09
2025-12-18 00:56:58,793 - root - INFO - Step: 355 | Loss (Avg): 6.43 | Reserved Memory 35.23 GB  | Tokens per second: 52834.17 | Training tokens per second (%): 8.85 | MFU (%): 8.07 | TFLOP/s/GPU: 79.78
2025-12-18 00:57:04,999 - root - INFO - Step: 360 | Loss (Avg): 6.42 | Reserved Memory 35.23 GB  | Tokens per second: 52818.14 | Training tokens per second (%): 7.85 | MFU (%): 8.06 | TFLOP/s/GPU: 79.76
2025-12-18 00:57:11,190 - root - INFO - Step: 365 | Loss (Avg): 6.41 | Reserved Memory 35.23 GB  | Tokens per second: 52937.52 | Training tokens per second (%): 10.22 | MFU (%): 8.08 | TFLOP/s/GPU: 79.94
2025-12-18 00:57:17,399 - root - INFO - Step: 370 | Loss (Avg): 6.51 | Reserved Memory 35.23 GB  | Tokens per second: 52787.85 | Training tokens per second (%): 9.56 | MFU (%): 8.06 | TFLOP/s/GPU: 79.71
2025-12-18 00:57:23,805 - root - INFO - Step: 375 | Loss (Avg): 6.39 | Reserved Memory 35.23 GB  | Tokens per second: 51160.89 | Training tokens per second (%): 10.19 | MFU (%): 7.81 | TFLOP/s/GPU: 77.25
2025-12-18 00:57:30,131 - root - INFO - Step: 380 | Loss (Avg): 6.40 | Reserved Memory 35.23 GB  | Tokens per second: 51809.84 | Training tokens per second (%): 11.07 | MFU (%): 7.91 | TFLOP/s/GPU: 78.23
2025-12-18 00:57:36,342 - root - INFO - Step: 385 | Loss (Avg): 6.22 | Reserved Memory 35.23 GB  | Tokens per second: 52763.12 | Training tokens per second (%): 8.84 | MFU (%): 8.06 | TFLOP/s/GPU: 79.67
2025-12-18 00:57:42,512 - root - INFO - Step: 390 | Loss (Avg): 6.36 | Reserved Memory 35.23 GB  | Tokens per second: 53125.86 | Training tokens per second (%): 8.91 | MFU (%): 8.11 | TFLOP/s/GPU: 80.22
2025-12-18 00:57:48,690 - root - INFO - Step: 395 | Loss (Avg): 6.48 | Reserved Memory 35.23 GB  | Tokens per second: 53048.24 | Training tokens per second (%): 8.57 | MFU (%): 8.10 | TFLOP/s/GPU: 80.10
2025-12-18 00:57:54,755 - root - INFO - Step: 400 | Loss (Avg): 6.41 | Reserved Memory 35.23 GB  | Tokens per second: 54036.69 | Training tokens per second (%): 10.40 | MFU (%): 8.25 | TFLOP/s/GPU: 81.60
2025-12-18 00:58:00,791 - root - INFO - Step: 405 | Loss (Avg): 6.54 | Reserved Memory 35.23 GB  | Tokens per second: 54298.64 | Training tokens per second (%): 9.65 | MFU (%): 8.29 | TFLOP/s/GPU: 81.99
2025-12-18 00:58:06,822 - root - INFO - Step: 410 | Loss (Avg): 6.33 | Reserved Memory 35.23 GB  | Tokens per second: 54345.92 | Training tokens per second (%): 9.42 | MFU (%): 8.30 | TFLOP/s/GPU: 82.06
2025-12-18 00:58:13,270 - root - INFO - Step: 415 | Loss (Avg): 6.36 | Reserved Memory 35.23 GB  | Tokens per second: 50823.72 | Training tokens per second (%): 8.51 | MFU (%): 7.76 | TFLOP/s/GPU: 76.74
2025-12-18 00:58:19,460 - root - INFO - Step: 420 | Loss (Avg): 6.34 | Reserved Memory 35.23 GB  | Tokens per second: 52953.25 | Training tokens per second (%): 9.40 | MFU (%): 8.08 | TFLOP/s/GPU: 79.96
2025-12-18 00:58:25,580 - root - INFO - Step: 425 | Loss (Avg): 6.23 | Reserved Memory 35.23 GB  | Tokens per second: 53552.65 | Training tokens per second (%): 10.51 | MFU (%): 8.18 | TFLOP/s/GPU: 80.86
2025-12-18 00:58:31,603 - root - INFO - Step: 430 | Loss (Avg): 6.34 | Reserved Memory 35.23 GB  | Tokens per second: 54410.19 | Training tokens per second (%): 8.59 | MFU (%): 8.31 | TFLOP/s/GPU: 82.16
2025-12-18 00:58:37,615 - root - INFO - Step: 435 | Loss (Avg): 6.48 | Reserved Memory 35.23 GB  | Tokens per second: 54512.42 | Training tokens per second (%): 7.98 | MFU (%): 8.32 | TFLOP/s/GPU: 82.31
2025-12-18 00:58:43,640 - root - INFO - Step: 440 | Loss (Avg): 6.40 | Reserved Memory 35.23 GB  | Tokens per second: 54402.23 | Training tokens per second (%): 9.70 | MFU (%): 8.31 | TFLOP/s/GPU: 82.15
2025-12-18 00:58:49,669 - root - INFO - Step: 445 | Loss (Avg): 6.30 | Reserved Memory 35.23 GB  | Tokens per second: 54356.35 | Training tokens per second (%): 11.33 | MFU (%): 8.30 | TFLOP/s/GPU: 82.08
2025-12-18 00:58:56,094 - root - INFO - Step: 450 | Loss (Avg): 6.45 | Reserved Memory 35.23 GB  | Tokens per second: 51012.76 | Training tokens per second (%): 9.90 | MFU (%): 7.79 | TFLOP/s/GPU: 77.03
2025-12-18 00:59:02,152 - root - INFO - Step: 455 | Loss (Avg): 6.47 | Reserved Memory 35.23 GB  | Tokens per second: 54105.78 | Training tokens per second (%): 8.19 | MFU (%): 8.26 | TFLOP/s/GPU: 81.70
2025-12-18 00:59:08,194 - root - INFO - Step: 460 | Loss (Avg): 6.40 | Reserved Memory 35.23 GB  | Tokens per second: 54245.62 | Training tokens per second (%): 11.38 | MFU (%): 8.28 | TFLOP/s/GPU: 81.91
2025-12-18 00:59:14,394 - root - INFO - Step: 465 | Loss (Avg): 6.50 | Reserved Memory 35.23 GB  | Tokens per second: 52857.69 | Training tokens per second (%): 11.77 | MFU (%): 8.07 | TFLOP/s/GPU: 79.82
2025-12-18 00:59:20,427 - root - INFO - Step: 470 | Loss (Avg): 6.44 | Reserved Memory 35.23 GB  | Tokens per second: 54325.85 | Training tokens per second (%): 10.55 | MFU (%): 8.29 | TFLOP/s/GPU: 82.03
2025-12-18 00:59:26,432 - root - INFO - Step: 475 | Loss (Avg): 6.40 | Reserved Memory 35.23 GB  | Tokens per second: 54578.87 | Training tokens per second (%): 10.95 | MFU (%): 8.33 | TFLOP/s/GPU: 82.41
2025-12-18 00:59:32,450 - root - INFO - Step: 480 | Loss (Avg): 6.35 | Reserved Memory 35.23 GB  | Tokens per second: 54460.61 | Training tokens per second (%): 10.52 | MFU (%): 8.32 | TFLOP/s/GPU: 82.24
2025-12-18 00:59:38,457 - root - INFO - Step: 485 | Loss (Avg): 6.48 | Reserved Memory 35.23 GB  | Tokens per second: 54566.03 | Training tokens per second (%): 8.43 | MFU (%): 8.33 | TFLOP/s/GPU: 82.39
2025-12-18 00:59:44,967 - root - INFO - Step: 490 | Loss (Avg): 6.23 | Reserved Memory 35.23 GB  | Tokens per second: 50343.89 | Training tokens per second (%): 8.96 | MFU (%): 7.69 | TFLOP/s/GPU: 76.02
2025-12-18 00:59:50,955 - root - INFO - Step: 495 | Loss (Avg): 6.35 | Reserved Memory 35.23 GB  | Tokens per second: 54732.77 | Training tokens per second (%): 10.20 | MFU (%): 8.36 | TFLOP/s/GPU: 82.65
2025-12-18 00:59:56,962 - root - INFO - Step: 500 | Loss (Avg): 6.34 | Reserved Memory 35.23 GB  | Tokens per second: 54559.99 | Training tokens per second (%): 9.44 | MFU (%): 8.33 | TFLOP/s/GPU: 82.39
2025-12-18 01:00:03,166 - root - INFO - Step: 505 | Loss (Avg): 6.30 | Reserved Memory 35.23 GB  | Tokens per second: 52831.42 | Training tokens per second (%): 9.87 | MFU (%): 8.07 | TFLOP/s/GPU: 79.78
2025-12-18 01:00:09,193 - root - INFO - Step: 510 | Loss (Avg): 6.20 | Reserved Memory 35.23 GB  | Tokens per second: 54379.03 | Training tokens per second (%): 10.78 | MFU (%): 8.30 | TFLOP/s/GPU: 82.11
2025-12-18 01:00:15,222 - root - INFO - Step: 515 | Loss (Avg): 6.14 | Reserved Memory 35.23 GB  | Tokens per second: 54357.06 | Training tokens per second (%): 8.55 | MFU (%): 8.30 | TFLOP/s/GPU: 82.08
2025-12-18 01:00:21,215 - root - INFO - Step: 520 | Loss (Avg): 6.35 | Reserved Memory 35.23 GB  | Tokens per second: 54691.47 | Training tokens per second (%): 9.57 | MFU (%): 8.35 | TFLOP/s/GPU: 82.58
2025-12-18 01:00:27,646 - root - INFO - Step: 525 | Loss (Avg): 6.45 | Reserved Memory 35.23 GB  | Tokens per second: 50966.44 | Training tokens per second (%): 11.89 | MFU (%): 7.78 | TFLOP/s/GPU: 76.96
2025-12-18 01:00:33,815 - root - INFO - Step: 530 | Loss (Avg): 6.27 | Reserved Memory 35.23 GB  | Tokens per second: 53126.53 | Training tokens per second (%): 8.36 | MFU (%): 8.11 | TFLOP/s/GPU: 80.22
2025-12-18 01:00:39,931 - root - INFO - Step: 535 | Loss (Avg): 6.32 | Reserved Memory 35.23 GB  | Tokens per second: 53586.65 | Training tokens per second (%): 8.06 | MFU (%): 8.18 | TFLOP/s/GPU: 80.92
2025-12-18 01:00:45,937 - root - INFO - Step: 540 | Loss (Avg): 6.29 | Reserved Memory 35.23 GB  | Tokens per second: 54577.50 | Training tokens per second (%): 8.98 | MFU (%): 8.33 | TFLOP/s/GPU: 82.41
2025-12-18 01:00:51,935 - root - INFO - Step: 545 | Loss (Avg): 6.22 | Reserved Memory 35.23 GB  | Tokens per second: 54633.61 | Training tokens per second (%): 10.02 | MFU (%): 8.34 | TFLOP/s/GPU: 82.50
2025-12-18 01:00:57,960 - root - INFO - Step: 550 | Loss (Avg): 6.21 | Reserved Memory 35.23 GB  | Tokens per second: 54404.83 | Training tokens per second (%): 10.28 | MFU (%): 8.31 | TFLOP/s/GPU: 82.15
2025-12-18 01:01:03,952 - root - INFO - Step: 555 | Loss (Avg): 6.32 | Reserved Memory 35.23 GB  | Tokens per second: 54693.08 | Training tokens per second (%): 10.52 | MFU (%): 8.35 | TFLOP/s/GPU: 82.59
2025-12-18 01:01:10,327 - root - INFO - Step: 560 | Loss (Avg): 6.31 | Reserved Memory 35.23 GB  | Tokens per second: 51415.06 | Training tokens per second (%): 7.80 | MFU (%): 7.85 | TFLOP/s/GPU: 77.64
2025-12-18 01:01:16,577 - root - INFO - Step: 565 | Loss (Avg): 6.35 | Reserved Memory 35.23 GB  | Tokens per second: 52434.42 | Training tokens per second (%): 10.40 | MFU (%): 8.01 | TFLOP/s/GPU: 79.18
2025-12-18 01:01:22,576 - root - INFO - Step: 570 | Loss (Avg): 6.41 | Reserved Memory 35.23 GB  | Tokens per second: 54630.99 | Training tokens per second (%): 9.93 | MFU (%): 8.34 | TFLOP/s/GPU: 82.49
2025-12-18 01:01:28,581 - root - INFO - Step: 575 | Loss (Avg): 6.13 | Reserved Memory 35.23 GB  | Tokens per second: 54583.48 | Training tokens per second (%): 8.65 | MFU (%): 8.33 | TFLOP/s/GPU: 82.42
2025-12-18 01:01:34,750 - root - INFO - Step: 580 | Loss (Avg): 6.36 | Reserved Memory 35.23 GB  | Tokens per second: 53122.47 | Training tokens per second (%): 11.66 | MFU (%): 8.11 | TFLOP/s/GPU: 80.21
2025-12-18 01:01:40,763 - root - INFO - Step: 585 | Loss (Avg): 6.35 | Reserved Memory 35.23 GB  | Tokens per second: 54509.09 | Training tokens per second (%): 10.54 | MFU (%): 8.32 | TFLOP/s/GPU: 82.31
2025-12-18 01:01:46,754 - root - INFO - Step: 590 | Loss (Avg): 6.31 | Reserved Memory 35.23 GB  | Tokens per second: 54707.10 | Training tokens per second (%): 11.10 | MFU (%): 8.35 | TFLOP/s/GPU: 82.61
2025-12-18 01:01:52,778 - root - INFO - Step: 595 | Loss (Avg): 6.23 | Reserved Memory 35.23 GB  | Tokens per second: 54404.64 | Training tokens per second (%): 9.19 | MFU (%): 8.31 | TFLOP/s/GPU: 82.15
2025-12-18 01:01:58,777 - root - INFO - Step: 600 | Loss (Avg): 6.15 | Reserved Memory 35.23 GB  | Tokens per second: 54641.88 | Training tokens per second (%): 9.26 | MFU (%): 8.34 | TFLOP/s/GPU: 82.51
2025-12-18 01:02:05,343 - root - INFO - Step: 605 | Loss (Avg): 6.17 | Reserved Memory 35.23 GB  | Tokens per second: 49912.12 | Training tokens per second (%): 10.97 | MFU (%): 7.62 | TFLOP/s/GPU: 75.37
2025-12-18 01:02:11,547 - root - INFO - Step: 610 | Loss (Avg): 6.29 | Reserved Memory 35.23 GB  | Tokens per second: 52831.11 | Training tokens per second (%): 10.16 | MFU (%): 8.07 | TFLOP/s/GPU: 79.77
2025-12-18 01:02:17,569 - root - INFO - Step: 615 | Loss (Avg): 6.19 | Reserved Memory 35.23 GB  | Tokens per second: 54421.33 | Training tokens per second (%): 9.32 | MFU (%): 8.31 | TFLOP/s/GPU: 82.18
2025-12-18 01:02:23,561 - root - INFO - Step: 620 | Loss (Avg): 6.18 | Reserved Memory 35.23 GB  | Tokens per second: 54698.53 | Training tokens per second (%): 10.30 | MFU (%): 8.35 | TFLOP/s/GPU: 82.59
2025-12-18 01:02:29,581 - root - INFO - Step: 625 | Loss (Avg): 6.27 | Reserved Memory 35.23 GB  | Tokens per second: 54440.36 | Training tokens per second (%): 8.74 | MFU (%): 8.31 | TFLOP/s/GPU: 82.20
2025-12-18 01:02:35,629 - root - INFO - Step: 630 | Loss (Avg): 6.12 | Reserved Memory 35.23 GB  | Tokens per second: 54190.33 | Training tokens per second (%): 9.81 | MFU (%): 8.27 | TFLOP/s/GPU: 81.83
2025-12-18 01:02:41,856 - root - INFO - Step: 635 | Loss (Avg): 6.05 | Reserved Memory 35.23 GB  | Tokens per second: 52639.54 | Training tokens per second (%): 10.87 | MFU (%): 8.04 | TFLOP/s/GPU: 79.49
2025-12-18 01:02:48,154 - root - INFO - Step: 640 | Loss (Avg): 6.30 | Reserved Memory 35.23 GB  | Tokens per second: 52033.07 | Training tokens per second (%): 10.46 | MFU (%): 7.94 | TFLOP/s/GPU: 78.57
2025-12-18 01:02:54,146 - root - INFO - Step: 645 | Loss (Avg): 6.24 | Reserved Memory 35.23 GB  | Tokens per second: 54702.75 | Training tokens per second (%): 9.51 | MFU (%): 8.35 | TFLOP/s/GPU: 82.60
2025-12-18 01:03:00,060 - root - INFO - Step: 650 | Loss (Avg): 6.28 | Reserved Memory 35.23 GB  | Tokens per second: 55422.38 | Training tokens per second (%): 9.64 | MFU (%): 8.46 | TFLOP/s/GPU: 83.69
2025-12-18 01:03:06,025 - root - INFO - Step: 655 | Loss (Avg): 6.23 | Reserved Memory 35.23 GB  | Tokens per second: 54942.70 | Training tokens per second (%): 11.42 | MFU (%): 8.39 | TFLOP/s/GPU: 82.96
2025-12-18 01:03:11,952 - root - INFO - Step: 660 | Loss (Avg): 6.28 | Reserved Memory 35.23 GB  | Tokens per second: 55299.48 | Training tokens per second (%): 10.15 | MFU (%): 8.44 | TFLOP/s/GPU: 83.50
2025-12-18 01:03:17,865 - root - INFO - Step: 665 | Loss (Avg): 6.25 | Reserved Memory 35.23 GB  | Tokens per second: 55420.90 | Training tokens per second (%): 10.16 | MFU (%): 8.46 | TFLOP/s/GPU: 83.69
2025-12-18 01:03:23,777 - root - INFO - Step: 670 | Loss (Avg): 6.15 | Reserved Memory 35.23 GB  | Tokens per second: 55442.86 | Training tokens per second (%): 9.64 | MFU (%): 8.46 | TFLOP/s/GPU: 83.72
2025-12-18 01:03:29,931 - root - INFO - Step: 675 | Loss (Avg): 6.29 | Reserved Memory 35.23 GB  | Tokens per second: 53259.65 | Training tokens per second (%): 9.62 | MFU (%): 8.13 | TFLOP/s/GPU: 80.42
2025-12-18 01:03:35,976 - root - INFO - Step: 680 | Loss (Avg): 6.19 | Reserved Memory 35.23 GB  | Tokens per second: 54214.72 | Training tokens per second (%): 7.86 | MFU (%): 8.28 | TFLOP/s/GPU: 81.86
2025-12-18 01:03:41,965 - root - INFO - Step: 685 | Loss (Avg): 6.25 | Reserved Memory 35.23 GB  | Tokens per second: 54725.00 | Training tokens per second (%): 10.93 | MFU (%): 8.36 | TFLOP/s/GPU: 82.63
2025-12-18 01:03:48,087 - root - INFO - Step: 690 | Loss (Avg): 6.23 | Reserved Memory 35.23 GB  | Tokens per second: 53536.25 | Training tokens per second (%): 9.76 | MFU (%): 8.17 | TFLOP/s/GPU: 80.84
2025-12-18 01:03:54,010 - root - INFO - Step: 695 | Loss (Avg): 6.14 | Reserved Memory 35.23 GB  | Tokens per second: 55338.39 | Training tokens per second (%): 9.28 | MFU (%): 8.45 | TFLOP/s/GPU: 83.56
2025-12-18 01:03:59,909 - root - INFO - Step: 700 | Loss (Avg): 6.22 | Reserved Memory 35.23 GB  | Tokens per second: 55552.04 | Training tokens per second (%): 9.46 | MFU (%): 8.48 | TFLOP/s/GPU: 83.88
2025-12-18 01:04:06,065 - root - INFO - Step: 705 | Loss (Avg): 6.19 | Reserved Memory 35.23 GB  | Tokens per second: 53246.26 | Training tokens per second (%): 9.32 | MFU (%): 8.13 | TFLOP/s/GPU: 80.40
2025-12-18 01:04:11,998 - root - INFO - Step: 710 | Loss (Avg): 6.29 | Reserved Memory 35.23 GB  | Tokens per second: 55235.13 | Training tokens per second (%): 9.44 | MFU (%): 8.43 | TFLOP/s/GPU: 83.41
2025-12-18 01:04:17,884 - root - INFO - Step: 715 | Loss (Avg): 6.30 | Reserved Memory 35.23 GB  | Tokens per second: 55686.48 | Training tokens per second (%): 9.01 | MFU (%): 8.50 | TFLOP/s/GPU: 84.09
2025-12-18 01:04:24,016 - root - INFO - Step: 720 | Loss (Avg): 6.19 | Reserved Memory 35.23 GB  | Tokens per second: 53449.72 | Training tokens per second (%): 9.43 | MFU (%): 8.16 | TFLOP/s/GPU: 80.71
2025-12-18 01:04:30,171 - root - INFO - Step: 725 | Loss (Avg): 6.20 | Reserved Memory 35.23 GB  | Tokens per second: 53248.46 | Training tokens per second (%): 11.61 | MFU (%): 8.13 | TFLOP/s/GPU: 80.41
2025-12-18 01:04:36,095 - root - INFO - Step: 730 | Loss (Avg): 6.11 | Reserved Memory 35.23 GB  | Tokens per second: 55327.17 | Training tokens per second (%): 8.54 | MFU (%): 8.45 | TFLOP/s/GPU: 83.54
2025-12-18 01:04:41,985 - root - INFO - Step: 735 | Loss (Avg): 6.22 | Reserved Memory 35.23 GB  | Tokens per second: 55638.27 | Training tokens per second (%): 7.87 | MFU (%): 8.49 | TFLOP/s/GPU: 84.01
2025-12-18 01:04:47,894 - root - INFO - Step: 740 | Loss (Avg): 6.09 | Reserved Memory 35.23 GB  | Tokens per second: 55469.33 | Training tokens per second (%): 10.62 | MFU (%): 8.47 | TFLOP/s/GPU: 83.76
2025-12-18 01:04:54,093 - root - INFO - Step: 745 | Loss (Avg): 6.21 | Reserved Memory 35.23 GB  | Tokens per second: 52868.83 | Training tokens per second (%): 8.82 | MFU (%): 8.07 | TFLOP/s/GPU: 79.83
2025-12-18 01:05:00,077 - root - INFO - Step: 750 | Loss (Avg): 6.10 | Reserved Memory 35.23 GB  | Tokens per second: 54768.31 | Training tokens per second (%): 12.54 | MFU (%): 8.36 | TFLOP/s/GPU: 82.70
2025-12-18 01:05:06,226 - root - INFO - Step: 755 | Loss (Avg): 6.19 | Reserved Memory 35.23 GB  | Tokens per second: 53304.83 | Training tokens per second (%): 9.03 | MFU (%): 8.14 | TFLOP/s/GPU: 80.49
2025-12-18 01:05:12,315 - root - INFO - Step: 760 | Loss (Avg): 6.07 | Reserved Memory 35.23 GB  | Tokens per second: 53828.48 | Training tokens per second (%): 10.24 | MFU (%): 8.22 | TFLOP/s/GPU: 81.28
2025-12-18 01:05:18,233 - root - INFO - Step: 765 | Loss (Avg): 6.20 | Reserved Memory 35.23 GB  | Tokens per second: 55380.31 | Training tokens per second (%): 9.97 | MFU (%): 8.46 | TFLOP/s/GPU: 83.62
2025-12-18 01:05:24,220 - root - INFO - Step: 770 | Loss (Avg): 6.13 | Reserved Memory 35.23 GB  | Tokens per second: 54737.25 | Training tokens per second (%): 9.74 | MFU (%): 8.36 | TFLOP/s/GPU: 82.65
2025-12-18 01:05:30,137 - root - INFO - Step: 775 | Loss (Avg): 6.37 | Reserved Memory 35.23 GB  | Tokens per second: 55393.84 | Training tokens per second (%): 9.91 | MFU (%): 8.46 | TFLOP/s/GPU: 83.64
2025-12-18 01:05:36,326 - root - INFO - Step: 780 | Loss (Avg): 6.34 | Reserved Memory 35.23 GB  | Tokens per second: 52952.97 | Training tokens per second (%): 9.01 | MFU (%): 8.08 | TFLOP/s/GPU: 79.96
2025-12-18 01:05:42,251 - root - INFO - Step: 785 | Loss (Avg): 6.30 | Reserved Memory 35.23 GB  | Tokens per second: 55325.00 | Training tokens per second (%): 9.76 | MFU (%): 8.45 | TFLOP/s/GPU: 83.54
2025-12-18 01:05:48,185 - root - INFO - Step: 790 | Loss (Avg): 6.07 | Reserved Memory 35.23 GB  | Tokens per second: 55230.90 | Training tokens per second (%): 12.09 | MFU (%): 8.43 | TFLOP/s/GPU: 83.40
2025-12-18 01:05:54,340 - root - INFO - Step: 795 | Loss (Avg): 6.17 | Reserved Memory 35.23 GB  | Tokens per second: 53243.50 | Training tokens per second (%): 11.37 | MFU (%): 8.13 | TFLOP/s/GPU: 80.40
2025-12-18 01:06:00,501 - root - INFO - Step: 800 | Loss (Avg): 6.16 | Reserved Memory 35.23 GB  | Tokens per second: 53200.77 | Training tokens per second (%): 11.05 | MFU (%): 8.12 | TFLOP/s/GPU: 80.33
2025-12-18 01:06:06,403 - root - INFO - Step: 805 | Loss (Avg): 6.29 | Reserved Memory 35.23 GB  | Tokens per second: 55529.73 | Training tokens per second (%): 9.77 | MFU (%): 8.48 | TFLOP/s/GPU: 83.85
2025-12-18 01:06:12,322 - root - INFO - Step: 810 | Loss (Avg): 6.21 | Reserved Memory 35.23 GB  | Tokens per second: 55380.74 | Training tokens per second (%): 9.04 | MFU (%): 8.46 | TFLOP/s/GPU: 83.62
2025-12-18 01:06:18,511 - root - INFO - Step: 815 | Loss (Avg): 6.22 | Reserved Memory 35.23 GB  | Tokens per second: 52947.22 | Training tokens per second (%): 11.13 | MFU (%): 8.08 | TFLOP/s/GPU: 79.95
2025-12-18 01:06:24,421 - root - INFO - Step: 820 | Loss (Avg): 6.12 | Reserved Memory 35.23 GB  | Tokens per second: 55457.00 | Training tokens per second (%): 8.97 | MFU (%): 8.47 | TFLOP/s/GPU: 83.74
2025-12-18 01:06:30,468 - root - INFO - Step: 825 | Loss (Avg): 6.27 | Reserved Memory 35.23 GB  | Tokens per second: 54199.63 | Training tokens per second (%): 10.36 | MFU (%): 8.28 | TFLOP/s/GPU: 81.84
2025-12-18 01:06:36,461 - root - INFO - Step: 830 | Loss (Avg): 6.24 | Reserved Memory 35.23 GB  | Tokens per second: 54692.67 | Training tokens per second (%): 9.70 | MFU (%): 8.35 | TFLOP/s/GPU: 82.59
2025-12-18 01:06:42,349 - root - INFO - Step: 835 | Loss (Avg): 6.15 | Reserved Memory 35.23 GB  | Tokens per second: 55658.42 | Training tokens per second (%): 11.44 | MFU (%): 8.50 | TFLOP/s/GPU: 84.04
2025-12-18 01:06:48,395 - root - INFO - Step: 840 | Loss (Avg): 6.07 | Reserved Memory 35.23 GB  | Tokens per second: 54217.51 | Training tokens per second (%): 8.50 | MFU (%): 8.28 | TFLOP/s/GPU: 81.87
2025-12-18 01:06:54,434 - root - INFO - Step: 845 | Loss (Avg): 6.18 | Reserved Memory 35.23 GB  | Tokens per second: 54270.40 | Training tokens per second (%): 11.62 | MFU (%): 8.29 | TFLOP/s/GPU: 81.95
2025-12-18 01:07:00,377 - root - INFO - Step: 850 | Loss (Avg): 6.11 | Reserved Memory 35.23 GB  | Tokens per second: 55146.59 | Training tokens per second (%): 10.94 | MFU (%): 8.42 | TFLOP/s/GPU: 83.27
2025-12-18 01:07:06,328 - root - INFO - Step: 855 | Loss (Avg): 6.07 | Reserved Memory 35.23 GB  | Tokens per second: 55072.49 | Training tokens per second (%): 11.95 | MFU (%): 8.41 | TFLOP/s/GPU: 83.16
2025-12-18 01:07:12,540 - root - INFO - Step: 860 | Loss (Avg): 5.96 | Reserved Memory 35.23 GB  | Tokens per second: 52761.32 | Training tokens per second (%): 10.26 | MFU (%): 8.06 | TFLOP/s/GPU: 79.67
2025-12-18 01:07:18,518 - root - INFO - Step: 865 | Loss (Avg): 6.16 | Reserved Memory 35.23 GB  | Tokens per second: 54826.67 | Training tokens per second (%): 11.13 | MFU (%): 8.37 | TFLOP/s/GPU: 82.79
2025-12-18 01:07:24,439 - root - INFO - Step: 870 | Loss (Avg): 6.04 | Reserved Memory 35.23 GB  | Tokens per second: 55350.24 | Training tokens per second (%): 12.65 | MFU (%): 8.45 | TFLOP/s/GPU: 83.58
2025-12-18 01:07:30,523 - root - INFO - Step: 875 | Loss (Avg): 6.18 | Reserved Memory 35.23 GB  | Tokens per second: 53872.59 | Training tokens per second (%): 10.01 | MFU (%): 8.23 | TFLOP/s/GPU: 81.35
2025-12-18 01:07:36,656 - root - INFO - Step: 880 | Loss (Avg): 6.08 | Reserved Memory 35.23 GB  | Tokens per second: 53438.27 | Training tokens per second (%): 10.48 | MFU (%): 8.16 | TFLOP/s/GPU: 80.69
2025-12-18 01:07:42,645 - root - INFO - Step: 885 | Loss (Avg): 6.11 | Reserved Memory 35.23 GB  | Tokens per second: 54731.65 | Training tokens per second (%): 9.58 | MFU (%): 8.36 | TFLOP/s/GPU: 82.64
2025-12-18 01:07:48,577 - root - INFO - Step: 890 | Loss (Avg): 6.19 | Reserved Memory 35.23 GB  | Tokens per second: 55245.15 | Training tokens per second (%): 9.14 | MFU (%): 8.43 | TFLOP/s/GPU: 83.42
2025-12-18 01:07:54,726 - root - INFO - Step: 895 | Loss (Avg): 6.12 | Reserved Memory 35.23 GB  | Tokens per second: 53301.10 | Training tokens per second (%): 8.24 | MFU (%): 8.14 | TFLOP/s/GPU: 80.48
2025-12-18 01:08:00,679 - root - INFO - Step: 900 | Loss (Avg): 6.10 | Reserved Memory 35.23 GB  | Tokens per second: 55056.75 | Training tokens per second (%): 9.53 | MFU (%): 8.41 | TFLOP/s/GPU: 83.14
2025-12-18 01:08:06,618 - root - INFO - Step: 905 | Loss (Avg): 6.10 | Reserved Memory 35.23 GB  | Tokens per second: 55180.33 | Training tokens per second (%): 8.31 | MFU (%): 8.42 | TFLOP/s/GPU: 83.32
2025-12-18 01:08:12,687 - root - INFO - Step: 910 | Loss (Avg): 6.13 | Reserved Memory 35.23 GB  | Tokens per second: 54000.14 | Training tokens per second (%): 9.20 | MFU (%): 8.24 | TFLOP/s/GPU: 81.54
2025-12-18 01:08:18,603 - root - INFO - Step: 915 | Loss (Avg): 6.15 | Reserved Memory 35.23 GB  | Tokens per second: 55404.06 | Training tokens per second (%): 6.86 | MFU (%): 8.46 | TFLOP/s/GPU: 83.66
2025-12-18 01:08:24,742 - root - INFO - Step: 920 | Loss (Avg): 5.98 | Reserved Memory 35.23 GB  | Tokens per second: 53382.90 | Training tokens per second (%): 9.71 | MFU (%): 8.15 | TFLOP/s/GPU: 80.61
2025-12-18 01:08:30,675 - root - INFO - Step: 925 | Loss (Avg): 5.99 | Reserved Memory 35.23 GB  | Tokens per second: 55247.60 | Training tokens per second (%): 7.85 | MFU (%): 8.44 | TFLOP/s/GPU: 83.42
2025-12-18 01:08:36,580 - root - INFO - Step: 930 | Loss (Avg): 6.00 | Reserved Memory 35.23 GB  | Tokens per second: 55502.92 | Training tokens per second (%): 10.08 | MFU (%): 8.47 | TFLOP/s/GPU: 83.81
2025-12-18 01:08:42,526 - root - INFO - Step: 935 | Loss (Avg): 6.15 | Reserved Memory 35.23 GB  | Tokens per second: 55115.81 | Training tokens per second (%): 9.43 | MFU (%): 8.42 | TFLOP/s/GPU: 83.22
2025-12-18 01:08:48,432 - root - INFO - Step: 940 | Loss (Avg): 6.25 | Reserved Memory 35.23 GB  | Tokens per second: 55498.62 | Training tokens per second (%): 8.97 | MFU (%): 8.47 | TFLOP/s/GPU: 83.80
2025-12-18 01:08:54,712 - root - INFO - Step: 945 | Loss (Avg): 6.07 | Reserved Memory 35.23 GB  | Tokens per second: 52190.24 | Training tokens per second (%): 10.60 | MFU (%): 7.97 | TFLOP/s/GPU: 78.81
2025-12-18 01:09:00,727 - root - INFO - Step: 950 | Loss (Avg): 5.88 | Reserved Memory 35.23 GB  | Tokens per second: 54488.25 | Training tokens per second (%): 11.91 | MFU (%): 8.32 | TFLOP/s/GPU: 82.28
2025-12-18 01:09:06,651 - root - INFO - Step: 955 | Loss (Avg): 6.11 | Reserved Memory 35.23 GB  | Tokens per second: 55320.87 | Training tokens per second (%): 9.80 | MFU (%): 8.45 | TFLOP/s/GPU: 83.53
2025-12-18 01:09:12,672 - root - INFO - Step: 960 | Loss (Avg): 6.13 | Reserved Memory 35.23 GB  | Tokens per second: 54439.62 | Training tokens per second (%): 7.73 | MFU (%): 8.31 | TFLOP/s/GPU: 82.20
2025-12-18 01:09:18,631 - root - INFO - Step: 965 | Loss (Avg): 5.99 | Reserved Memory 35.23 GB  | Tokens per second: 54997.28 | Training tokens per second (%): 8.10 | MFU (%): 8.40 | TFLOP/s/GPU: 83.05
2025-12-18 01:09:24,581 - root - INFO - Step: 970 | Loss (Avg): 5.98 | Reserved Memory 35.23 GB  | Tokens per second: 55080.51 | Training tokens per second (%): 8.55 | MFU (%): 8.41 | TFLOP/s/GPU: 83.17
2025-12-18 01:09:30,465 - root - INFO - Step: 975 | Loss (Avg): 5.93 | Reserved Memory 35.23 GB  | Tokens per second: 55706.67 | Training tokens per second (%): 8.48 | MFU (%): 8.51 | TFLOP/s/GPU: 84.12
2025-12-18 01:09:36,645 - root - INFO - Step: 980 | Loss (Avg): 5.93 | Reserved Memory 35.23 GB  | Tokens per second: 53034.71 | Training tokens per second (%): 10.06 | MFU (%): 8.10 | TFLOP/s/GPU: 80.08
2025-12-18 01:09:42,704 - root - INFO - Step: 985 | Loss (Avg): 6.04 | Reserved Memory 35.23 GB  | Tokens per second: 54091.05 | Training tokens per second (%): 8.61 | MFU (%): 8.26 | TFLOP/s/GPU: 81.68
2025-12-18 01:09:48,983 - root - INFO - Step: 990 | Loss (Avg): 6.10 | Reserved Memory 35.23 GB  | Tokens per second: 52196.53 | Training tokens per second (%): 8.14 | MFU (%): 7.97 | TFLOP/s/GPU: 78.82
2025-12-18 01:09:54,894 - root - INFO - Step: 995 | Loss (Avg): 6.10 | Reserved Memory 35.23 GB  | Tokens per second: 55448.96 | Training tokens per second (%): 8.35 | MFU (%): 8.47 | TFLOP/s/GPU: 83.73
2025-12-18 01:10:00,897 - root - INFO - Step: 1000 | Loss (Avg): 6.04 | Reserved Memory 35.23 GB  | Tokens per second: 54612.31 | Training tokens per second (%): 8.71 | MFU (%): 8.34 | TFLOP/s/GPU: 82.46
2025-12-18 01:10:00,949 - root - INFO - Training completed
2025-12-18 01:10:00,951 - root - INFO - Training completed
2025-12-18 01:10:00,952 - root - INFO - Training completed
2025-12-18 01:10:00,954 - root - INFO - Training completed
2025-12-18 01:10:00,958 - root - INFO - Training completed
2025-12-18 01:10:00,959 - root - INFO - Training completed
2025-12-18 01:10:00,963 - root - INFO - Training completed
2025-12-18 01:10:00,963 - root - INFO - Training completed
2025-12-18 01:10:00,966 - root - INFO - Training completed
2025-12-18 01:10:00,969 - root - INFO - Training completed
2025-12-18 01:10:00,969 - root - INFO - Training completed
2025-12-18 01:10:00,971 - root - INFO - Training completed
2025-12-18 01:10:00,971 - root - INFO - Training completed
2025-12-18 01:10:00,971 - root - INFO - Training completed
2025-12-18 01:10:00,972 - root - INFO - Training completed
2025-12-18 01:10:00,972 - root - INFO - Training completed
2025-12-18 01:10:00,974 - root - INFO - Training completed
2025-12-18 01:10:00,977 - root - INFO - Training completed
2025-12-18 01:10:00,979 - root - INFO - Training completed
2025-12-18 01:10:00,979 - root - INFO - Training completed
2025-12-18 01:10:00,982 - root - INFO - Training completed
2025-12-18 01:10:00,985 - root - INFO - Training completed
2025-12-18 01:10:00,985 - root - INFO - Training completed
2025-12-18 01:10:00,985 - root - INFO - Training completed
2025-12-18 01:10:00,990 - root - INFO - Training completed
2025-12-18 01:10:00,998 - root - INFO - Training completed
2025-12-18 01:10:01,001 - root - INFO - Training completed
2025-12-18 01:10:01,004 - root - INFO - Training completed
2025-12-18 01:10:01,005 - root - INFO - Training completed
2025-12-18 01:10:01,005 - root - INFO - Training completed
2025-12-18 01:10:01,006 - root - INFO - Training completed
2025-12-18 01:10:01,010 - root - INFO - Training completed
END TIME: Thu Dec 18 01:10:05 CET 2025
[sbatch-master] task finished
