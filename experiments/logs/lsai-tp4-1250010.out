START TIME: Wed Dec 17 02:28:45 CET 2025
[sbatch-master] running on nid007182
[sbatch-master] SLURM_NODELIST: nid007182
[sbatch-master] SLURM_NNODES: 1
[sbatch-master] SLURM_NODEID: 0
[sbatch-master] define some env vars that will be passed to the compute nodes
[sbatch-master] execute command on compute nodes
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
[srun] rank=0 host=nid007182 noderank=0 localrank=0
W1217 02:28:55.888000 219194 torch/distributed/run.py:792] 
W1217 02:28:55.888000 219194 torch/distributed/run.py:792] *****************************************
W1217 02:28:55.888000 219194 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1217 02:28:55.888000 219194 torch/distributed/run.py:792] *****************************************
2025-12-17 02:29:01,096 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=4, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=200, logging_frequency=5, checkpoint_frequency=100, checkpoint_save_path=None, checkpoint_load_path=None, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False, tp_size=4)
2025-12-17 02:29:01,096 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=4, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=200, logging_frequency=5, checkpoint_frequency=100, checkpoint_save_path=None, checkpoint_load_path=None, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False, tp_size=4)
2025-12-17 02:29:01,096 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=4, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=200, logging_frequency=5, checkpoint_frequency=100, checkpoint_save_path=None, checkpoint_load_path=None, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False, tp_size=4)
Setting device to local rank: 1
Setting device to local rank: 2
Setting device to local rank: 3
Setting device to local rank: 0
2025-12-17 02:29:01,096 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=4, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=200, logging_frequency=5, checkpoint_frequency=100, checkpoint_save_path=None, checkpoint_load_path=None, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False, tp_size=4)
[Rank 0] World Size: 4, DP: 0 / 1, TP: 0 / 4
2025-12-17 02:29:07,857 - root - INFO - Setting up DataLoaders...
[Rank 1] World Size: 4, DP: 0 / 1, TP: 1 / 4
2025-12-17 02:29:08,331 - root - INFO - Setting up DataLoaders...
[Rank 3] World Size: 4, DP: 0 / 1, TP: 3 / 4
2025-12-17 02:29:08,360 - root - INFO - Setting up DataLoaders...
[Rank 2] World Size: 4, DP: 0 / 1, TP: 2 / 4
2025-12-17 02:29:08,370 - root - INFO - Setting up DataLoaders...
2025-12-17 02:29:13,000 - root - INFO - Setting up Model...
2025-12-17 02:29:13,022 - root - INFO - Applying Tensor Parallelism with size 4...
2025-12-17 02:29:13,285 - root - INFO - Setting up Model...
2025-12-17 02:29:13,304 - root - INFO - Applying Tensor Parallelism with size 4...
2025-12-17 02:29:13,319 - root - INFO - Setting up Model...
2025-12-17 02:29:13,339 - root - INFO - Applying Tensor Parallelism with size 4...
2025-12-17 02:29:13,382 - root - INFO - Setting up Model...
2025-12-17 02:29:13,408 - root - INFO - Applying Tensor Parallelism with size 4...
[rank3]:[W1217 02:29:13.862223375 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank2]:[W1217 02:29:13.870069634 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank0]:[W1217 02:29:13.896623026 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank1]:[W1217 02:29:13.970650915 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-12-17 02:29:21,984 - root - INFO - Starting training!
2025-12-17 02:29:21,984 - root - INFO - Starting training!
2025-12-17 02:29:21,984 - root - INFO - Starting training!
2025-12-17 02:29:21,984 - root - INFO - Starting training!
2025-12-17 02:29:30,590 - root - INFO - Step: 1 | Loss (Avg): 11.96 | Reserved Memory 33.32 GB  | Tokens per second: 952.02 | Training tokens per second (%): 30.27 | MFU (%): 1.16 | TFLOP/s/GPU: 11.50
2025-12-17 02:29:35,752 - root - INFO - Step: 5 | Loss (Avg): 11.93 | Reserved Memory 42.85 GB  | Tokens per second: 6348.59 | Training tokens per second (%): 28.09 | MFU (%): 7.75 | TFLOP/s/GPU: 76.69
2025-12-17 02:29:41,826 - root - INFO - Step: 10 | Loss (Avg): 11.83 | Reserved Memory 42.85 GB  | Tokens per second: 6745.55 | Training tokens per second (%): 32.52 | MFU (%): 8.24 | TFLOP/s/GPU: 81.49
2025-12-17 02:29:47,150 - root - INFO - Step: 15 | Loss (Avg): 11.67 | Reserved Memory 42.85 GB  | Tokens per second: 7694.62 | Training tokens per second (%): 32.22 | MFU (%): 9.40 | TFLOP/s/GPU: 92.95
2025-12-17 02:29:52,557 - root - INFO - Step: 20 | Loss (Avg): 11.34 | Reserved Memory 42.85 GB  | Tokens per second: 7577.54 | Training tokens per second (%): 38.81 | MFU (%): 9.26 | TFLOP/s/GPU: 91.54
2025-12-17 02:29:57,920 - root - INFO - Step: 25 | Loss (Avg): 10.83 | Reserved Memory 42.85 GB  | Tokens per second: 7639.54 | Training tokens per second (%): 35.94 | MFU (%): 9.33 | TFLOP/s/GPU: 92.29
2025-12-17 02:30:03,531 - root - INFO - Step: 30 | Loss (Avg): 10.52 | Reserved Memory 42.85 GB  | Tokens per second: 7300.40 | Training tokens per second (%): 42.33 | MFU (%): 8.92 | TFLOP/s/GPU: 88.19
2025-12-17 02:30:07,872 - root - INFO - Step: 35 | Loss (Avg): 10.35 | Reserved Memory 42.85 GB  | Tokens per second: 9439.74 | Training tokens per second (%): 43.52 | MFU (%): 11.53 | TFLOP/s/GPU: 114.03
2025-12-17 02:30:12,215 - root - INFO - Step: 40 | Loss (Avg): 9.69 | Reserved Memory 42.85 GB  | Tokens per second: 9434.82 | Training tokens per second (%): 31.02 | MFU (%): 11.52 | TFLOP/s/GPU: 113.97
2025-12-17 02:30:16,559 - root - INFO - Step: 45 | Loss (Avg): 9.49 | Reserved Memory 42.85 GB  | Tokens per second: 9431.14 | Training tokens per second (%): 35.30 | MFU (%): 11.52 | TFLOP/s/GPU: 113.93
2025-12-17 02:30:20,943 - root - INFO - Step: 50 | Loss (Avg): 9.12 | Reserved Memory 42.85 GB  | Tokens per second: 9345.05 | Training tokens per second (%): 45.42 | MFU (%): 11.41 | TFLOP/s/GPU: 112.89
2025-12-17 02:30:25,288 - root - INFO - Step: 55 | Loss (Avg): 9.01 | Reserved Memory 42.85 GB  | Tokens per second: 9428.34 | Training tokens per second (%): 51.33 | MFU (%): 11.52 | TFLOP/s/GPU: 113.89
2025-12-17 02:30:29,667 - root - INFO - Step: 60 | Loss (Avg): 8.32 | Reserved Memory 42.85 GB  | Tokens per second: 9356.21 | Training tokens per second (%): 29.76 | MFU (%): 11.43 | TFLOP/s/GPU: 113.02
2025-12-17 02:30:34,030 - root - INFO - Step: 65 | Loss (Avg): 9.06 | Reserved Memory 42.85 GB  | Tokens per second: 9390.27 | Training tokens per second (%): 36.39 | MFU (%): 11.47 | TFLOP/s/GPU: 113.43
2025-12-17 02:30:38,647 - root - INFO - Step: 70 | Loss (Avg): 7.92 | Reserved Memory 42.85 GB  | Tokens per second: 8875.14 | Training tokens per second (%): 47.06 | MFU (%): 10.84 | TFLOP/s/GPU: 107.21
2025-12-17 02:30:43,020 - root - INFO - Step: 75 | Loss (Avg): 7.78 | Reserved Memory 42.85 GB  | Tokens per second: 9367.83 | Training tokens per second (%): 49.40 | MFU (%): 11.44 | TFLOP/s/GPU: 113.16
2025-12-17 02:30:47,408 - root - INFO - Step: 80 | Loss (Avg): 7.78 | Reserved Memory 42.85 GB  | Tokens per second: 9338.24 | Training tokens per second (%): 40.83 | MFU (%): 11.41 | TFLOP/s/GPU: 112.81
2025-12-17 02:30:51,831 - root - INFO - Step: 85 | Loss (Avg): 7.78 | Reserved Memory 42.85 GB  | Tokens per second: 9263.38 | Training tokens per second (%): 29.89 | MFU (%): 11.31 | TFLOP/s/GPU: 111.90
2025-12-17 02:30:56,229 - root - INFO - Step: 90 | Loss (Avg): 7.54 | Reserved Memory 42.85 GB  | Tokens per second: 9316.63 | Training tokens per second (%): 35.88 | MFU (%): 11.38 | TFLOP/s/GPU: 112.54
2025-12-17 02:31:00,622 - root - INFO - Step: 95 | Loss (Avg): 7.27 | Reserved Memory 42.85 GB  | Tokens per second: 9326.09 | Training tokens per second (%): 34.81 | MFU (%): 11.39 | TFLOP/s/GPU: 112.66
2025-12-17 02:31:04,994 - root - INFO - Step: 100 | Loss (Avg): 7.05 | Reserved Memory 42.85 GB  | Tokens per second: 9370.49 | Training tokens per second (%): 38.14 | MFU (%): 11.45 | TFLOP/s/GPU: 113.20
2025-12-17 02:31:09,393 - root - INFO - Step: 105 | Loss (Avg): 7.88 | Reserved Memory 42.85 GB  | Tokens per second: 9312.77 | Training tokens per second (%): 31.39 | MFU (%): 11.37 | TFLOP/s/GPU: 112.50
2025-12-17 02:31:13,771 - root - INFO - Step: 110 | Loss (Avg): 7.37 | Reserved Memory 42.85 GB  | Tokens per second: 9358.92 | Training tokens per second (%): 36.30 | MFU (%): 11.43 | TFLOP/s/GPU: 113.06
2025-12-17 02:31:18,431 - root - INFO - Step: 115 | Loss (Avg): 7.62 | Reserved Memory 42.85 GB  | Tokens per second: 8791.83 | Training tokens per second (%): 46.18 | MFU (%): 10.74 | TFLOP/s/GPU: 106.21
2025-12-17 02:31:22,831 - root - INFO - Step: 120 | Loss (Avg): 7.22 | Reserved Memory 42.85 GB  | Tokens per second: 9313.22 | Training tokens per second (%): 26.15 | MFU (%): 11.38 | TFLOP/s/GPU: 112.50
2025-12-17 02:31:27,242 - root - INFO - Step: 125 | Loss (Avg): 7.34 | Reserved Memory 42.85 GB  | Tokens per second: 9288.66 | Training tokens per second (%): 45.89 | MFU (%): 11.35 | TFLOP/s/GPU: 112.21
2025-12-17 02:31:31,661 - root - INFO - Step: 130 | Loss (Avg): 7.57 | Reserved Memory 42.85 GB  | Tokens per second: 9270.17 | Training tokens per second (%): 34.73 | MFU (%): 11.32 | TFLOP/s/GPU: 111.98
2025-12-17 02:31:36,071 - root - INFO - Step: 135 | Loss (Avg): 7.33 | Reserved Memory 42.85 GB  | Tokens per second: 9290.65 | Training tokens per second (%): 47.49 | MFU (%): 11.35 | TFLOP/s/GPU: 112.23
2025-12-17 02:31:40,491 - root - INFO - Step: 140 | Loss (Avg): 7.11 | Reserved Memory 42.85 GB  | Tokens per second: 9270.73 | Training tokens per second (%): 52.54 | MFU (%): 11.32 | TFLOP/s/GPU: 111.99
2025-12-17 02:31:44,908 - root - INFO - Step: 145 | Loss (Avg): 7.22 | Reserved Memory 42.85 GB  | Tokens per second: 9275.85 | Training tokens per second (%): 42.08 | MFU (%): 11.33 | TFLOP/s/GPU: 112.05
2025-12-17 02:31:49,317 - root - INFO - Step: 150 | Loss (Avg): 7.31 | Reserved Memory 42.85 GB  | Tokens per second: 9290.62 | Training tokens per second (%): 49.27 | MFU (%): 11.35 | TFLOP/s/GPU: 112.23
2025-12-17 02:31:54,144 - root - INFO - Step: 155 | Loss (Avg): 7.30 | Reserved Memory 42.85 GB  | Tokens per second: 8488.36 | Training tokens per second (%): 34.32 | MFU (%): 10.37 | TFLOP/s/GPU: 102.54
2025-12-17 02:31:58,546 - root - INFO - Step: 160 | Loss (Avg): 6.89 | Reserved Memory 42.85 GB  | Tokens per second: 9307.20 | Training tokens per second (%): 32.84 | MFU (%): 11.37 | TFLOP/s/GPU: 112.43
2025-12-17 02:32:02,959 - root - INFO - Step: 165 | Loss (Avg): 7.20 | Reserved Memory 42.85 GB  | Tokens per second: 9284.97 | Training tokens per second (%): 45.02 | MFU (%): 11.34 | TFLOP/s/GPU: 112.16
2025-12-17 02:32:07,365 - root - INFO - Step: 170 | Loss (Avg): 7.27 | Reserved Memory 42.85 GB  | Tokens per second: 9298.22 | Training tokens per second (%): 29.93 | MFU (%): 11.36 | TFLOP/s/GPU: 112.32
2025-12-17 02:32:11,790 - root - INFO - Step: 175 | Loss (Avg): 7.11 | Reserved Memory 42.85 GB  | Tokens per second: 9260.49 | Training tokens per second (%): 43.14 | MFU (%): 11.31 | TFLOP/s/GPU: 111.87
2025-12-17 02:32:16,220 - root - INFO - Step: 180 | Loss (Avg): 7.12 | Reserved Memory 42.85 GB  | Tokens per second: 9248.85 | Training tokens per second (%): 39.49 | MFU (%): 11.30 | TFLOP/s/GPU: 111.73
2025-12-17 02:32:20,651 - root - INFO - Step: 185 | Loss (Avg): 7.26 | Reserved Memory 42.85 GB  | Tokens per second: 9245.87 | Training tokens per second (%): 36.04 | MFU (%): 11.29 | TFLOP/s/GPU: 111.69
2025-12-17 02:32:25,081 - root - INFO - Step: 190 | Loss (Avg): 7.21 | Reserved Memory 42.85 GB  | Tokens per second: 9249.58 | Training tokens per second (%): 32.73 | MFU (%): 11.30 | TFLOP/s/GPU: 111.73
2025-12-17 02:32:29,930 - root - INFO - Step: 195 | Loss (Avg): 7.13 | Reserved Memory 42.85 GB  | Tokens per second: 8449.10 | Training tokens per second (%): 52.04 | MFU (%): 10.32 | TFLOP/s/GPU: 102.07
2025-12-17 02:32:34,351 - root - INFO - Step: 200 | Loss (Avg): 7.00 | Reserved Memory 42.85 GB  | Tokens per second: 9287.22 | Training tokens per second (%): 32.56 | MFU (%): 11.34 | TFLOP/s/GPU: 112.19
2025-12-17 02:32:34,407 - root - INFO - Training completed
2025-12-17 02:32:34,415 - root - INFO - Training completed
2025-12-17 02:32:34,416 - root - INFO - Training completed
2025-12-17 02:32:34,423 - root - INFO - Training completed
END TIME: Wed Dec 17 02:32:39 CET 2025
[sbatch-master] task finished
